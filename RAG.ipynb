{"cells":[{"cell_type":"markdown","metadata":{"id":"aKCrXO7N8340"},"source":["Retrievel augmented generation\n","\n","1. load pdfs\n","2. convert text into vectors\n","3."]},{"cell_type":"markdown","metadata":{"id":"Anzr4X1A8345"},"source":["testing"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12406,"status":"ok","timestamp":1707572530535,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"9W2R-V0y9Y7l","outputId":"e1759603-7072-4617-ec23-9ee29fcb182a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: langchain in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (0.1.6)\n","Requirement already satisfied: PyYAML>=5.3 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from langchain) (2.0.25)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from langchain) (3.9.3)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from langchain) (4.0.3)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from langchain) (0.6.4)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from langchain) (1.33)\n","Requirement already satisfied: langchain-community<0.1,>=0.0.18 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from langchain) (0.0.19)\n","Requirement already satisfied: langchain-core<0.2,>=0.1.22 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from langchain) (0.1.22)\n","Requirement already satisfied: langsmith<0.1,>=0.0.83 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from langchain) (0.0.87)\n","Requirement already satisfied: numpy<2,>=1 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3,>=1 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from langchain) (2.6.1)\n","Requirement already satisfied: requests<3,>=2 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from langchain) (8.2.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n","Requirement already satisfied: anyio<5,>=3 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.22->langchain) (4.2.0)\n","Requirement already satisfied: packaging<24.0,>=23.2 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.22->langchain) (23.2)\n","Requirement already satisfied: annotated-types>=0.4.0 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.2 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (2.16.2)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.2.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: sniffio>=1.1 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.3.0)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.2.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install langchain"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9153,"status":"ok","timestamp":1707572539684,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"pZ1eOrlU97ay"},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install --upgrade --quiet  langchain-google-genai"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1707572539684,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"dsGMth_49_y1"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from langchain_google_genai import GoogleGenerativeAI"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1707572539684,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"C_VQ_qnW-HAW"},"outputs":[],"source":["llm = GoogleGenerativeAI(model=\"models/text-bison-001\", google_api_key=api_key,temperature =0.5)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":698,"status":"ok","timestamp":1707572540377,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"S7m93TaO8346","outputId":"3a17bb25-4506-45cc-a19f-dcbbab347fb3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n","  warn_deprecated(\n"]},{"name":"stdout","output_type":"stream","text":["New Delhi\n"]}],"source":["query = llm(\"what is the capital of india?\")\n","print(query)"]},{"cell_type":"markdown","metadata":{"id":"Vaa1Yc9y8346"},"source":["import files"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1707572540378,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"vdfz9hr18346"},"outputs":[],"source":["from langchain_community.document_loaders import PyPDFDirectoryLoader"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9348,"status":"ok","timestamp":1707572549723,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"9mp0fJ3qAING","outputId":"bfaaa028-3d15-4c8f-e945-a1dd1d552b1d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pypdf in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (4.0.1)\n","Requirement already satisfied: typing_extensions>=3.7.4.3 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from pypdf) (4.9.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install pypdf"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3708,"status":"ok","timestamp":1707572553426,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"Y4JPh3nd8346"},"outputs":[],"source":["loader = PyPDFDirectoryLoader(\"data/\")\n","docs = loader.load()\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1707572553426,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"6cWyWkYw8346","outputId":"7ac1314f-af4a-4990-a71a-0edee9401240"},"outputs":[{"data":{"text/plain":["[Document(page_content='Deep Learning for Deepfakes Creation and Detection: A Survey\\nThanh Thi Nguyena, Quoc Viet Hung Nguyenb, Dung Tien Nguyena, Duc Thanh Nguyena, Thien Huynh-Thec,\\nSaeid Nahavandid, Thanh Tam Nguyene, Quoc-Viet Phamf, Cuong M. Nguyeng\\naSchool of Information Technology, Deakin University, Victoria, Australia\\nbSchool of Information and Communication Technology, Gri ﬃth University, Queensland, Australia\\ncICT Convergence Research Center, Kumoh National Institute of Technology, Gyeongbuk, Republic of Korea\\ndInstitute for Intelligent Systems Research and Innovation, Deakin University, Victoria, Australia\\neFaculty of Information Technology, Ho Chi Minh City University of Technology (HUTECH), Ho Chi Minh City, Vietnam\\nfKorean Southeast Center for the 4th Industrial Revolution Leader Education, Pusan National University, Busan, Republic of Korea\\ngLAMIH UMR CNRS 8201, Universite Polytechnique Hauts-de-France, Valenciennes, France\\nAbstract\\nDeep learning has been successfully applied to solve various complex problems ranging from big data analytics\\nto computer vision and human-level control. Deep learning advances however have also been employed to create\\nsoftware that can cause threats to privacy, democracy and national security. One of those deep learning-powered\\napplications recently emerged is deepfake. Deepfake algorithms can create fake images and videos that humans\\ncannot distinguish them from authentic ones. The proposal of technologies that can automatically detect and assess\\nthe integrity of digital visual media is therefore indispensable. This paper presents a survey of algorithms used to\\ncreate deepfakes and, more importantly, methods proposed to detect deepfakes in the literature to date. We present\\nextensive discussions on challenges, research trends and directions related to deepfake technologies. By reviewing\\nthe background of deepfakes and state-of-the-art deepfake detection methods, this study provides a comprehensive\\noverview of deepfake techniques and facilitates the development of new and more robust methods to deal with the\\nincreasingly challenging deepfakes.\\nKeywords: deepfakes, face manipulation, artiﬁcial intelligence, deep learning, autoencoders, GAN, forensics, survey\\n1. Introduction\\nIn a narrow deﬁnition, deepfakes (stemming from\\n“deep learning” and “fake”) are created by techniques\\nthat can superimpose face images of a target person onto\\na video of a source person to make a video of the target\\nperson doing or saying things the source person does.\\nThis constitutes a category of deepfakes, namely face-\\nswap . In a broader deﬁnition, deepfakes are artiﬁcial\\nintelligence-synthesized content that can also fall into\\ntwo other categories, i.e., lip-sync andpuppet-master .\\nLip-sync deepfakes refer to videos that are modiﬁed to\\nmake the mouth movements consistent with an audio\\nrecording. Puppet-master deepfakes include videos of\\na target person (puppet) who is animated following the\\nfacial expressions, eye and head movements of another\\nperson (master) sitting in front of a camera [1].\\nWhile some deepfakes can be created by traditional\\nvisual e ﬀects or computer-graphics approaches, the re-\\ncent common underlying mechanism for deepfake cre-\\nation is deep learning models such as autoencoders andgenerative adversarial networks (GANs), which have\\nbeen applied widely in the computer vision domain [2–\\n8]. These models are used to examine facial expressions\\nand movements of a person and synthesize facial images\\nof another person making analogous expressions and\\nmovements [9]. Deepfake methods normally require a\\nlarge amount of image and video data to train models\\nto create photo-realistic images and videos. As pub-\\nlic ﬁgures such as celebrities and politicians may have\\na large number of videos and images available online,\\nthey are initial targets of deepfakes. Deepfakes were\\nused to swap faces of celebrities or politicians to bod-\\nies in porn images and videos. The ﬁrst deepfake video\\nemerged in 2017 where face of a celebrity was swapped\\nto the face of a porn actor. It is threatening to world se-\\ncurity when deepfake methods can be employed to cre-\\nate videos of world leaders with fake speeches for falsi-\\nﬁcation purposes [10–12]. Deepfakes therefore can be\\nabused to cause political or religion tensions between\\ncountries, to fool public and a ﬀect results in electionarXiv:1909.11573v5  [cs.CV]  11 Aug 2022', metadata={'source': 'data/1909.11573.pdf', 'page': 0}),\n"," Document(page_content='campaigns, or create chaos in ﬁnancial markets by cre-\\nating fake news [13–15]. It can be even used to generate\\nfake satellite images of the Earth to contain objects that\\ndo not really exist to confuse military analysts, e.g., cre-\\nating a fake bridge across a river although there is no\\nsuch a bridge in reality. This can mislead a troop who\\nhave been guided to cross the bridge in a battle [16, 17].\\nAs the democratization of creating realistic digital\\nhumans has positive implications, there is also positive\\nuse of deepfakes such as their applications in visual ef-\\nfects, digital avatars, snapchat ﬁlters, creating voices\\nof those who have lost theirs or updating episodes of\\nmovies without reshooting them [18]. Deepfakes can\\nhave creative or productive impacts in photography,\\nvideo games, virtual reality, movie productions, and\\nentertainment, e.g., realistic video dubbing of foreign\\nﬁlms, education through the reanimation of historical\\nﬁgures, virtually trying on clothes while shopping, and\\nso on [19, 20]. However, the number of malicious uses\\nof deepfakes largely dominates that of the positive ones.\\nThe development of advanced deep neural networks and\\nthe availability of large amount of data have made the\\nforged images and videos almost indistinguishable to\\nhumans and even to sophisticated computer algorithms.\\nThe process of creating those manipulated images and\\nvideos is also much simpler today as it needs as little\\nas an identity photo or a short video of a target individ-\\nual. Less and less e ﬀort is required to produce a stun-\\nningly convincing tempered footage. Recent advances\\ncan even create a deepfake with just a still image [21].\\nDeepfakes therefore can be a threat a ﬀecting not only\\npublic ﬁgures but also ordinary people. For example, a\\nvoice deepfake was used to scam a CEO out of $243,000\\n[22]. A recent release of a software called DeepNude\\nshows more disturbing threats as it can transform a per-\\nson to a non-consensual porn [23]. Likewise, the Chi-\\nnese app Zao has gone viral lately as less-skilled users\\ncan swap their faces onto bodies of movie stars and in-\\nsert themselves into well-known movies and TV clips\\n[24]. These forms of falsiﬁcation create a huge threat\\nto violation of privacy and identity, and a ﬀect many as-\\npects of human lives.\\nFinding the truth in digital domain therefore has be-\\ncome increasingly critical. It is even more challenging\\nwhen dealing with deepfakes as they are majorly used\\nto serve malicious purposes and almost anyone can cre-\\nate deepfakes these days using existing deepfake tools.\\nThus far, there have been numerous methods proposed\\nto detect deepfakes [25–29]. Most of them are based on\\ndeep learning, and thus a battle between malicious and\\npositive uses of deep learning methods has been aris-\\ning. To address the threat of face-swapping technology\\nFig. 1. Number of papers related to deepfakes in years from 2016 to\\n2021, obtained from https: //app.dimensions.ai at the end of 2021 with\\nthe search keyword “deepfake” applied to full text of scholarly papers.\\nor deepfakes, the United States Defense Advanced Re-\\nsearch Projects Agency (DARPA) initiated a research\\nscheme in media forensics (named Media Forensics or\\nMediFor) to accelerate the development of fake digital\\nvisual media detection methods [30]. Recently, Face-\\nbook Inc. teaming up with Microsoft Corp and the Part-\\nnership on AI coalition have launched the Deepfake De-\\ntection Challenge to catalyse more research and devel-\\nopment in detecting and preventing deepfakes from be-\\ning used to mislead viewers [31]. Data obtained from\\nhttps: //app.dimensions.ai at the end of 2021 show that\\nthe number of deepfake papers has increased signiﬁ-\\ncantly in recent years (Fig. 1). Although the obtained\\nnumbers of deepfake papers may be lower than actual\\nnumbers but the research trend of this topic is obviously\\nincreasing.\\nThere have been existing survey papers about creat-\\ning and detecting deepfakes, presented in [19, 20, 32].\\nFor example, Mirsky and Lee [19] focused on reen-\\nactment approaches (i.e., to change a target’s expres-\\nsion, mouth, pose, gaze or body), and replacement\\napproaches (i.e., to replace a target’s face by swap\\nor transfer methods). Verdoliva [20] separated detec-\\ntion approaches into conventional methods (e.g., blind\\nmethods without using any external data for train-\\ning, one-class sensor-based and model-based methods,\\nand supervised methods with handcrafted features) and\\ndeep learning-based approaches (e.g., CNN models).\\nTolosana et al. [32] categorized both creation and detec-\\ntion methods based on the way deepfakes are created,\\nincluding entire face synthesis, identity swap, attribute\\nmanipulation, and expression swap. On the other hand,\\nwe carry out the survey with a di ﬀerent perspective and\\ntaxonomy. We categorize the deepfake detection meth-\\n2', metadata={'source': 'data/1909.11573.pdf', 'page': 1}),\n"," Document(page_content='Fig. 2. Categories of reviewed papers relevant to deepfake detection\\nmethods where we divide papers into two major groups, i.e., fake im-\\nage detection and face video detection.\\nods based on the data type, i.e., images or videos, as\\npresented in Fig. 2. With fake image detection methods,\\nwe focus on the features that are used, i.e., whether they\\nare handcrafted features or deep features. With fake\\nvideo detection methods, two main subcategories are\\nidentiﬁed based on whether the method uses temporal\\nfeatures across frames or visual artifacts within a video\\nframe. We also discuss extensively the challenges, re-\\nsearch trends and directions on deepfake detection and\\nmultimedia forensics problems.\\n2. Deepfake Creation\\nDeepfakes have become popular due to the quality\\nof tampered videos and also the easy-to-use ability of\\ntheir applications to a wide range of users with vari-\\nous computer skills from professional to novice. These\\napplications are mostly developed based on deep learn-\\ning techniques. Deep learning is well known for its ca-\\npability of representing complex and high-dimensional\\ndata. One variant of the deep networks with that ca-\\npability is deep autoencoders, which have been widely\\napplied for dimensionality reduction and image com-\\npression [33–35]. The ﬁrst attempt of deepfake cre-\\nation was FakeApp, developed by a Reddit user using\\nautoencoder-decoder pairing structure [36, 37]. In that\\nmethod, the autoencoder extracts latent features of face\\nimages and the decoder is used to reconstruct the face\\nimages. To swap faces between source images and tar-\\nget images, there is a need of two encoder-decoder pairs\\nwhere each pair is used to train on an image set, and\\nthe encoder’s parameters are shared between two net-\\nwork pairs. In other words, two pairs have the same\\nencoder network. This strategy enables the common en-\\ncoder to ﬁnd and learn the similarity between two sets\\nof face images, which are relatively unchallenging be-\\nFig. 3. A deepfake creation model using two encoder-decoder pairs.\\nTwo networks use the same encoder but di ﬀerent decoders for train-\\ning process (top). An image of face A is encoded with the common\\nencoder and decoded with decoder B to create a deepfake (bottom).\\nThe reconstructed image (in the bottom) is the face B with the mouth\\nshape of face A. Face B originally has the mouth of an upside-down\\nheart while the reconstructed face B has the mouth of a conventional\\nheart.\\ncause faces normally have similar features such as eyes,\\nnose, mouth positions. Fig. 3 shows a deepfake cre-\\nation process where the feature set of face A is con-\\nnected with the decoder B to reconstruct face B from\\nthe original face A. This approach is applied in several\\nworks such as DeepFaceLab [38], DFaker [39], Deep-\\nFake tf (tensorﬂow-based deepfakes) [40].\\nBy adding adversarial loss and perceptual loss imple-\\nmented in VGGFace [57] to the encoder-decoder archi-\\ntecture, an improved version of deepfakes based on the\\ngenerative adversarial network [4], i.e., faceswap-GAN,\\nwas proposed in [58]. The VGGFace perceptual loss\\nis added to make eye movements to be more realistic\\nand consistent with input faces and help to smooth out\\nartifacts in segmentation mask, leading to higher qual-\\nity output videos. This model facilitates the creation of\\noutputs with 64x64, 128x128, and 256x256 resolutions.\\nIn addition, the multi-task convolutional neural network\\n(CNN) from the FaceNet implementation [59] is used\\nto make face detection more stable and face alignment\\nmore reliable. The CycleGAN [60] is utilized for gen-\\nerative network implementation in this model.\\nA conventional GAN model comprises two neural\\nnetworks: a generator and a discriminator as depicted in\\nFig. 4. Given a dataset of real images xhaving a distri-\\nbution of pdata, the aim of the generator Gis to produce\\nimages G(z) similar to real images xwith zbeing noise\\nsignals having a distribution of pz. The aim of the dis-\\ncriminator Gis to correctly classify images generated\\n3', metadata={'source': 'data/1909.11573.pdf', 'page': 2}),\n"," Document(page_content='Table 1: Summary of notable deepfake tools\\nTools Links Key Features\\nFaceswap https: //github.com /deepfakes /faceswap - Using two encoder-decoder pairs.\\n- Parameters of the encoder are shared.\\nFaceswap-GAN https: //github.com /shaoanlu /faceswap-GAN Adversarial loss and perceptual loss (VGGface) are added to an auto-encoder architecture.\\nFew-Shot Face\\nTranslationhttps: //github.com /shaoanlu /fewshot-face-\\ntranslation-GAN- Use a pre-trained face recognition model to extract latent embeddings for GAN process-\\ning.\\n- Incorporate semantic priors obtained by modules from FUNIT [41] and SPADE [42].\\nDeepFaceLab https: //github.com /iperov /DeepFaceLab - Expand from the Faceswap method with new models, e.g. H64, H128, LIAEF128, SAE\\n[43].\\n- Support multiple face extraction modes, e.g. S3FD, MTCNN, dlib, or manual [43].\\nDFaker https: //github.com /dfaker /df - DSSIM loss function [44] is used to reconstruct face.\\n- Implemented based on Keras library.\\nDeepFake tf https: //github.com /StromWine /DeepFake tf Similar to DFaker but implemented based on tensorﬂow.\\nAvatarMe https: //github.com /lattas /AvatarMe - Reconstruct 3D faces from arbitrary “in-the-wild” images.\\n- Can reconstruct authentic 4K by 6K-resolution 3D faces from a single low-resolution\\nimage [45].\\nMarioNETte https: //hyperconnect.github.io /MarioNETte - A few-shot face reenactment framework that preserves the target identity.\\n- No additional ﬁne-tuning phase is needed for identity adaptation [46].\\nDiscoFaceGAN https: //github.com /microsoft /DiscoFaceGAN - Generate face images of virtual people with independent latent variables of identity, ex-\\npression, pose, and illumination.\\n- Embed 3D priors into adversarial learning [47].\\nStyleRig https: //gvv.mpi-inf.mpg.de /projects /StyleRig - Create portrait images of faces with a rig-like control over a pretrained and ﬁxed Style-\\nGAN via 3D morphable face models.\\n- Self-supervised without manual annotations [48].\\nFaceShifter https: //lingzhili.com /FaceShifterPage - Face swapping in high-ﬁdelity by exploiting and integrating the target attributes.\\n- Can be applied to any new face pairs without requiring subject speciﬁc training [49].\\nFSGAN https: //github.com /YuvalNirkin /fsgan - A face swapping and reenactment model that can be applied to pairs of faces without\\nrequiring training on those faces.\\n- Adjust to both pose and expression variations [50].\\nStyleGAN https: //github.com /NVlabs /stylegan - A new generator architecture for GANs is proposed based on style transfer literature.\\n- The new architecture leads to automatic, unsupervised separation of high-level attributes\\nand enables intuitive, scale-speciﬁc control of the synthesis of images [51].\\nFace2Face https: //justusthies.github.io /posts/face2face / - Real-time facial reenactment of monocular target video sequence, e.g. Youtube video.\\n- Animate the facial expressions of the target video by a source actor and re-render the\\nmanipulated output video in a photo-realistic fashion [52].\\nNeural Textures https: //github.com /SSRSGJYD /NeuralTexture - Feature maps that are learned as part of the scene capture process and stored as maps on\\ntop of 3D mesh proxies.\\n- Can coherently re-render or manipulate existing video content in both static and dynamic\\nenvironments at real-time rates [53].\\nTransformable\\nBottleneck\\nNetworkshttps: //github.com /kyleolsz /TB-Networks - A method for ﬁne-grained 3D manipulation of image content.\\n- Apply spatial transformations in CNN models using a transformable bottleneck frame-\\nwork [54].\\n“Do as I Do”\\nMotion\\nTransfergithub.com /carolineec /EverybodyDanceNow - Automatically transfer the motion from a source to a target person by learning a video-\\nto-video translation.\\n- Can create a motion-synchronized dancing video with multiple subjects [55].\\nNeural V oice\\nPuppetryhttps: //justusthies.github.io /posts/neural-voice-\\npuppetry- A method for audio-driven facial video synthesis.\\n- Synthesize videos of a talking head from an audio sequence of another person using 3D\\nface representation. [56].\\n4', metadata={'source': 'data/1909.11573.pdf', 'page': 3}),\n"," Document(page_content='Fig. 4. The GAN architecture consisting of a generator and a discrim-\\ninator, and each can be implemented by a neural network. The entire\\nsystem can be trained with backpropagation that allows both networks\\nto improve their capabilities.\\nbyGand real images x. The discriminator Dis trained\\nto improve its classiﬁcation capability, i.e., to maximize\\nD(x), which represents the probability that xis a real\\nimage rather than a fake image generated by G. On the\\nother hand, Gis trained to minimize the probability that\\nits outputs are classiﬁed by Das synthetic images, i.e.,\\nto minimize 1−D(G(z)). This is a minimax game be-\\ntween two players DandGthat can be described by the\\nfollowing value function [4]:\\nmin\\nGmax\\nDV(D,G)=Ex∼pdata(x)[logD(x)]\\n+Ez∼pz(z)[log(1−D(G(z)))] (1)\\nAfter su ﬃcient training, both networks improve their\\ncapabilities, i.e., the generator Gis able to produce im-\\nages that are really similar to real images while the dis-\\ncriminator Dis highly capable of distinguishing fake\\nimages from real ones.\\nTable 1 presents a summary of popular deepfake tools\\nand their typical features. Among them, a prominent\\nmethod for face synthesis based on a GAN model,\\nnamely StyleGAN, was introduced in [51]. StyleGAN\\nis motivated by style transfer [61] with a special gen-\\nerator network architecture that is able to create realis-\\ntic face images. In a traditional GAN model, e.g., the\\nprogressive growing of GAN (PGGAN) [62], the signal\\nnoise (latent code) is fed to the input layer of a feed-\\nforward network that represents the generator. In Style-\\nGAN, there are two networks constructed and linked to-\\ngether, a mapping network fand a synthesis network g.\\nThe latent code z∈Zis ﬁrst converted to w∈W(where\\nWis an intermediate latent space) through a non-linear\\nfunction f:Z→W, which is characterized by a neural\\nnetwork (i.e., the mapping network) consisting of sev-eral fully connected layers. Using an a ﬃne tranforma-\\ntion, the intermediate representation wis specialized to\\nstyles y=(ys,yb) that will be fed to the adaptive in-\\nstance normalization (AdaIN) operations, speciﬁed as:\\nAdaIN( xi,y)=ys,ixi−µ(xi)\\nσ(xi)+yb,i (2)\\nwhere each feature map xiis normalized separately. The\\nStyleGAN generator architecture allows controlling the\\nimage synthesis by modifying the styles via di ﬀerent\\nscales. In addition, instead of using one random latent\\ncode during training, this method uses two latent codes\\nto generate a given proportion of images. More speciﬁ-\\ncally, two latent codes z1andz2are fed to the mapping\\nnetwork to create respectively w1andw2that control the\\nstyles by applying w1before and w2after the crossover\\npoint. Fig. 5 demonstrates examples of images cre-\\nated by mixing two latent codes at three di ﬀerent scales\\nwhere each subset of styles controls separate meaning-\\nful high-level attributes of the image. In other words,\\nthe generator architecture of StyleGAN is able to learn\\nseparation of high-level attributes (e.g., pose and iden-\\ntity when trained on human faces) and enables intuitive,\\nscale-speciﬁc control of the face synthesis.\\nFig. 5. Examples of mixing styles using StyleGAN: the output im-\\nages are generated by copying a speciﬁed subset of styles from source\\nB and taking the rest from source A. a) Copying coarse styles from\\nsource B will generate images that have high-level aspects from\\nsource B and all colors and ﬁner facial features from source A; b)\\nif copying the styles of middle resolutions from B, the output images\\nwill have smaller scale facial features from B and preserve the pose,\\ngeneral face shape, and eyeglasses from A; c) if copying the ﬁne styles\\nfrom source B, the generated images will have the color scheme and\\nmicrostructure of source B [51].\\n5', metadata={'source': 'data/1909.11573.pdf', 'page': 4}),\n"," Document(page_content='3. Deepfake Detection\\nDeepfake detection is normally deemed a binary clas-\\nsiﬁcation problem where classiﬁers are used to clas-\\nsify between authentic videos and tampered ones. This\\nkind of methods requires a large database of real and\\nfake videos to train classiﬁcation models. The num-\\nber of fake videos is increasingly available, but it is\\nstill limited in terms of setting a benchmark for vali-\\ndating various detection methods. To address this issue,\\nKorshunov and Marcel [63] produced a notable deep-\\nfake dataset consisting of 620 videos based on the GAN\\nmodel using the open source code Faceswap-GAN [58].\\nVideos from the publicly available VidTIMIT database\\n[64] were used to generate low and high quality deep-\\nfake videos, which can e ﬀectively mimic the facial ex-\\npressions, mouth movements, and eye blinking. These\\nvideos were then used to test various deepfake detection\\nmethods. Test results show that the popular face recog-\\nnition systems based on VGG [65] and Facenet [59, 66]\\nare unable to detect deepfakes e ﬀectively. Other meth-\\nods such as lip-syncing approaches [67–69] and im-\\nage quality metrics with support vector machine (SVM)\\n[70] produce very high error rate when applied to detect\\ndeepfake videos from this newly produced dataset. This\\nraises concerns about the critical need of future develop-\\nment of more robust methods that can detect deepfakes\\nfrom genuine.\\nThis section presents a survey of deepfake detection\\nmethods where we group them into two major cate-\\ngories: fake image detection methods and fake video\\ndetection ones (Fig. 2). The latter is distinguished\\ninto two smaller groups: visual artifacts within sin-\\ngle video frame-based methods and temporal features\\nacross frames-based ones. Whilst most of the methods\\nbased on temporal features use deep learning recurrent\\nclassiﬁcation models, the methods use visual artifacts\\nwithin video frame can be implemented by either deep\\nor shallow classiﬁers.\\n3.1. Fake Image Detection\\nDeepfakes are increasingly detrimental to privacy, so-\\nciety security and democracy [71]. Methods for detect-\\ning deepfakes have been proposed as soon as this threat\\nwas introduced. Early attempts were based on hand-\\ncrafted features obtained from artifacts and inconsisten-\\ncies of the fake image synthesis process. Recent meth-\\nods, e.g., [72, 73], have commonly applied deep learn-\\ning to automatically extract salient and discriminative\\nfeatures to detect deepfakes.3.1.1. Handcrafted Features-based Methods\\nMost works on detection of GAN generated images\\ndo not consider the generalization capability of the de-\\ntection models although the development of GAN is on-\\ngoing, and many new extensions of GAN are frequently\\nintroduced. Xuan et al. [74] used an image prepro-\\ncessing step, e.g., Gaussian blur and Gaussian noise,\\nto remove low level high frequency clues of GAN im-\\nages. This increases the pixel level statistical similar-\\nity between real images and fake images and allows the\\nforensic classiﬁer to learn more intrinsic and meaning-\\nful features, which has better generalization capability\\nthan previous image forensics methods [75, 76] or im-\\nage steganalysis networks [77].\\nZhang et al. [78] used the bag of words method to\\nextract a set of compact features and fed it into various\\nclassiﬁers such as SVM [79], random forest (RF) [80]\\nand multi-layer perceptrons (MLP) [81] for discriminat-\\ning swapped face images from the genuine. Among\\ndeep learning-generated images, those synthesised by\\nGAN models are probably most di ﬃcult to detect as\\nthey are realistic and high-quality based on GAN’s ca-\\npability to learn distribution of the complex input data\\nand generate new outputs with similar input distribu-\\ntion.\\nOn the other hand, Agarwal and Varshney [82] cast\\nthe GAN-based deepfake detection as a hypothesis test-\\ning problem where a statistical framework was intro-\\nduced using the information-theoretic study of authen-\\ntication [83]. The minimum distance between distribu-\\ntions of legitimate images and images generated by a\\nparticular GAN is deﬁned, namely the oracle error. The\\nanalytic results show that this distance increases when\\nthe GAN is less accurate, and in this case, it is easier\\nto detect deepfakes. In case of high-resolution image\\ninputs, an extremely accurate GAN is required to gen-\\nerate fake images that are hard to detect by this method.\\n3.1.2. Deep Features-based Methods\\nFace swapping has a number of compelling applica-\\ntions in video compositing, transﬁguration in portraits,\\nand especially in identity protection as it can replace\\nfaces in photographs by ones from a collection of stock\\nimages. However, it is also one of the techniques that\\ncyber attackers employ to penetrate identiﬁcation or au-\\nthentication systems to gain illegitimate access. The\\nuse of deep learning such as CNN and GAN has made\\nswapped face images more challenging for forensics\\nmodels as it can preserve pose, facial expression and\\nlighting of the photographs [84].\\nHsu et al. [85] introduced a two-phase deep learn-\\ning method for detection of deepfake images. The ﬁrst\\n6', metadata={'source': 'data/1909.11573.pdf', 'page': 5}),\n"," Document(page_content='phase is a feature extractor based on the common fake\\nfeature network (CFFN) where the Siamese network ar-\\nchitecture presented in [86] is used. The CFFN en-\\ncompasses several dense units with each unit including\\ndiﬀerent numbers of dense blocks [61] to improve the\\nrepresentative capability for the input images. Discrim-\\ninative features between the fake and real images are\\nextracted through the CFFN learning process based on\\nthe use of pairwise information, which is the label of\\neach pair of two input images. If the two images are\\nof the same type, i.e., fake-fake or real-real, the pair-\\nwise label is 1. In contrast, if they are of di ﬀerent types,\\ni.e., fake-real, the pairwise label is 0. The CFFN-based\\ndiscriminative features are then fed to a neural network\\nclassiﬁer to distinguish deceptive images from genuine.\\nThe proposed method is validated for both fake face\\nand fake general image detection. On the one hand,\\nthe face dataset is obtained from CelebA [87], contain-\\ning 10,177 identities and 202,599 aligned face images\\nof various poses and background clutter. Five GAN\\nvariants are used to generate fake images with size of\\n64x64, including deep convolutional GAN (DCGAN)\\n[88], Wasserstein GAN (WGAN) [89], WGAN with\\ngradient penalty (WGAN-GP) [90], least squares GAN\\n[91], and PGGAN [62]. A total of 385,198 training im-\\nages and 10,000 test images of both real and fake ones\\nare obtained for validating the proposed method. On\\nthe other hand, the general dataset is extracted from the\\nILSVRC12 [92]. The large scale GAN training model\\nfor high ﬁdelity natural image synthesis (BIGGAN)\\n[93], self-attention GAN [94] and spectral normaliza-\\ntion GAN [95] are used to generate fake images with\\nsize of 128x128. The training set consists of 600,000\\nfake and real images whilst the test set includes 10,000\\nimages of both types. Experimental results show the su-\\nperior performance of the proposed method against its\\ncompeting methods such as those introduced in [96–99].\\nLikewise, Guo et al. [100] proposed a CNN model,\\nnamely SCnet, to detect deepfake images, which are\\ngenerated by the Glow-based facial forgery tool [101].\\nThe fake images synthesized by the Glow model [101]\\nhave the facial expression maliciously tampered. These\\nimages are hyper-realistic with perfect visual qualities,\\nbut they still have subtle or noticeable manipulation\\ntraces, which are exploited by the SCnet. The SCnet is\\nable to automatically learn high-level forensics features\\nof image data thanks to a hierarchical feature extraction\\nblock, which is formed by stacking four convolutional\\nlayers. Each layer learns a new set of feature maps from\\nthe previous layer, with each convolutional operation isdeﬁned by:\\nf(n)\\nj=i∑\\ni=1f(n−1)\\ni∗ω(n)\\ni j+b(n)\\nj(3)\\nwhere f(n)\\njis the jthfeature map of the nthlayer,ω(n)\\ni j\\nis the weight of the ithchannel of the jthconvolutional\\nkernel in the nthlayer, and b(n)\\njis the bias term of\\nthejthconvolutional kernel in the nthlayer. The pro-\\nposed approach is evaluated using a dataset consisting\\nof 321,378 face images, which are created by applying\\nthe Glow model [101] to the CelebA face image dataset\\n[87]. Evaluation results show that the SCnet model ob-\\ntains higher accuracy and better generalization than the\\nMeso-4 model proposed in [102].\\nRecently, Zhao et al. [104] proposed a method\\nfor deepfake detection using self-consistency of lo-\\ncal source features, which are content-independent,\\nspatially-local information of images. These features\\ncould come from either imaging pipelines, encoding\\nmethods or image synthesis approaches. The hypothe-\\nsis is that a modiﬁed image would have di ﬀerent source\\nfeatures at di ﬀerent locations, while an original im-\\nage will have the same source features across loca-\\ntions. These source features, represented in the form\\nof down-sampled feature maps, are extracted by a CNN\\nmodel using a special representation learning method\\ncalled pairwise self-consistency learning. This learn-\\ning method aims to penalize pairs of feature vectors\\nthat refer to locations from the same image for hav-\\ning a low cosine similarity score. At the same time, it\\nalso penalizes the pairs from di ﬀerent images for hav-\\ning a high similarity score. The learned feature maps\\nare then fed to a classiﬁcation method for deepfake de-\\ntection. This proposed approach is evaluated on seven\\npopular datasets, including FaceForensics ++ [105],\\nDeepfakeDetection [106], Celeb-DF-v1 & Celeb-DF-\\nv2 [107], Deepfake Detection Challenge (DFDC) [108],\\nDFDC Preview [109], and DeeperForensics-1.0 [110].\\nExperimental results demonstrate that the proposed ap-\\nproach is superior to state-of-the-art methods. It how-\\never may have a limitation when dealing with fake im-\\nages that are generated by methods that directly output\\nthe whole images whose source features are consistent\\nacross all positions within each image.\\n3.2. Fake Video Detection\\nMost image detection methods cannot be used for\\nvideos because of the strong degradation of the frame\\ndata after video compression [102]. Furthermore,\\nvideos have temporal characteristics that are varied\\n7', metadata={'source': 'data/1909.11573.pdf', 'page': 6}),\n"," Document(page_content='Fig. 6. A two-step process for face manipulation detection where the preprocessing step aims to detect, crop and align faces on a sequence of\\nframes and the second step distinguishes manipulated and authentic face images by combining convolutional neural network (CNN) and recurrent\\nneural network (RNN) [103].\\namong sets of frames and they are thus challenging\\nfor methods designed to detect only still fake images.\\nThis subsection focuses on deepfake video detection\\nmethods and categorizes them into two smaller groups:\\nmethods that employ temporal features and those that\\nexplore visual artifacts within frames.\\n3.2.1. Temporal Features across Video Frames\\nBased on the observation that temporal coherence is\\nnot enforced e ﬀectively in the synthesis process of deep-\\nfakes, Sabir et al. [103] leveraged the use of spatio-\\ntemporal features of video streams to detect deepfakes.\\nVideo manipulation is carried out on a frame-by-frame\\nbasis so that low level artifacts produced by face ma-\\nnipulations are believed to further manifest themselves\\nas temporal artifacts with inconsistencies across frames.\\nA recurrent convolutional model (RCN) was proposed\\nbased on the integration of the convolutional network\\nDenseNet [61] and the gated recurrent unit cells [111] to\\nexploit temporal discrepancies across frames (see Fig.\\n6). The proposed method is tested on the FaceForen-\\nsics++dataset, which includes 1,000 videos [105], and\\nshows promising results.\\nLikewise, G ¨uera and Delp [112] highlighted that\\ndeepfake videos contain intra-frame inconsistencies and\\ntemporal inconsistencies between frames. They then\\nproposed the temporal-aware pipeline method that uses\\nCNN and long short term memory (LSTM) to detect\\ndeepfake videos. CNN is employed to extract frame-\\nlevel features, which are then fed into the LSTM to cre-\\nate a temporal sequence descriptor. A fully-connected\\nnetwork is ﬁnally used for classifying doctored videos\\nfrom real ones based on the sequence descriptor as il-\\nlustrated in Fig. 7. An accuracy of greater than 97%\\nwas obtained using a dataset of 600 videos, includ-\\ning 300 deepfake videos collected from multiple video-\\nhosting websites and 300 pristine videos randomly se-\\nlected from the Hollywood human actions dataset in\\n[113].\\nOn the other hand, the use of a physiological signal,\\neye blinking, to detect deepfakes was proposed in Li\\nFig. 7. A deepfake detection method using convolutional neural net-\\nwork (CNN) and long short term memory (LSTM) to extract tem-\\nporal features of a given video sequence, which are represented via\\nthe sequence descriptor. The detection network consisting of fully-\\nconnected layers is employed to take the sequence descriptor as input\\nand calculate probabilities of the frame sequence belonging to either\\nauthentic or deepfake class [112].\\net al. [114] based on the observation that a person in\\ndeepfakes has a lot less frequent blinking than that in\\nuntampered videos. A healthy adult human would nor-\\nmally blink somewhere between 2 to 10 seconds, and\\neach blink would take 0.1 and 0.4 seconds. Deepfake\\nalgorithms, however, often use face images available\\nonline for training, which normally show people with\\nopen eyes, i.e., very few images published on the inter-\\nnet show people with closed eyes. Thus, without hav-\\ning access to images of people blinking, deepfake algo-\\nrithms do not have the capability to generate fake faces\\nthat can blink normally. In other words, blinking rates in\\ndeepfakes are much lower than those in normal videos.\\nTo discriminate real and fake videos, Li et al. [114] crop\\neye areas in the videos and distribute them into long-\\nterm recurrent convolutional networks (LRCN) [115]\\nfor dynamic state prediction. The LRCN consists of\\na feature extractor based on CNN, a sequence learning\\nbased on long short term memory (LSTM), and a state\\nprediction based on a fully connected layer to predict\\nprobability of eye open and close state. The eye blink-\\ning shows strong temporal dependencies and thus the\\nimplementation of LSTM helps to capture these tempo-\\nral patterns e ﬀectively.\\nRecently, Caldelli et al. [116] proposed the use of\\noptical ﬂow to gauge the information along the tempo-\\nral axis of a frame sequence for video deepfake detec-\\ntion. The optical ﬂow is a vector ﬁeld calculated on two\\n8', metadata={'source': 'data/1909.11573.pdf', 'page': 7}),\n"," Document(page_content='temporal-distinct frames of a video that can describe the\\nmovement of objects in a scene. The optical ﬂow ﬁelds\\nare expected to be di ﬀerent between synthetically cre-\\nated frames and naturally generated ones [117]. Un-\\nnatural movements of lips, eyes, or of the entire faces\\ninserted into deepfake videos would introduce distinc-\\ntive motion patterns when compared with pristine ones.\\nBased on this assumption, features consisting of optical\\nﬂow ﬁelds are fed into a CNN model for discriminating\\nbetween deepfakes and original videos. More speciﬁ-\\ncally, the ResNet50 architecture [118] is implemented\\nas a CNN model for experiments. The results obtained\\nusing the FaceForensics ++dataset [105] show that this\\napproach is comparable with state-of-the-art methods in\\nterms of classiﬁcation accuracy. A combination of this\\nkind of feature with frame-based features is also exper-\\nimented, which results in an improved deepfake detec-\\ntion performance. This demonstrates the usefulness of\\noptical ﬂow ﬁelds in capturing the inconsistencies on\\nthe temporal axis of video frames for deepfake detec-\\ntion.\\n3.2.2. Visual Artifacts within Video Frame\\nAs can be noticed in the previous subsection, the\\nmethods using temporal patterns across video frames\\nare mostly based on deep recurrent network models to\\ndetect deepfake videos. This subsection investigates the\\nother approach that normally decomposes videos into\\nframes and explores visual artifacts within single frames\\nto obtain discriminant features. These features are then\\ndistributed into either a deep or shallow classiﬁer to dif-\\nferentiate between fake and authentic videos. We thus\\ngroup methods in this subsection based on the types of\\nclassiﬁers, i.e. either deep or shallow.\\nDeep classiﬁers. Deepfake videos are normally created\\nwith limited resolutions, which require an a ﬃne face\\nwarping approach (i.e., scaling, rotation and shearing)\\nto match the conﬁguration of the original ones. Because\\nof the resolution inconsistency between the warped face\\narea and the surrounding context, this process leaves\\nartifacts that can be detected by CNN models such as\\nVGG16 [119], ResNet50, ResNet101 and ResNet152\\n[118]. A deep learning method to detect deepfakes\\nbased on the artifacts observed during the face warp-\\ning step of the deepfake generation algorithms was pro-\\nposed in [120]. The proposed method is evaluated on\\ntwo deepfake datasets, namely the UADFV and Deep-\\nfakeTIMIT. The UADFV dataset [121] contains 49 real\\nvideos and 49 fake videos with 32,752 frames in to-\\ntal. The DeepfakeTIMIT dataset [69] includes a set of\\nlow quality videos of 64 x 64 size and another set ofhigh quality videos of 128 x 128 with totally 10,537\\npristine images and 34,023 fabricated images extracted\\nfrom 320 videos for each quality set. Performance of\\nthe proposed method is compared with other prevalent\\nmethods such as two deepfake detection MesoNet meth-\\nods, i.e. Meso-4 and MesoInception-4 [102], HeadPose\\n[121], and the face tampering detection method two-\\nstream NN [122]. Advantage of the proposed method\\nis that it needs not to generate deepfake videos as nega-\\ntive examples before training the detection models. In-\\nstead, the negative examples are generated dynamically\\nby extracting the face region of the original image and\\naligning it into multiple scales before applying Gaus-\\nsian blur to a scaled image of random pick and warping\\nback to the original image. This reduces a large amount\\nof time and computational resources compared to other\\nmethods, which require deepfakes are generated in ad-\\nvance.\\nNguyen et al. [123] proposed the use of capsule\\nnetworks for detecting manipulated images and videos.\\nThe capsule network was initially introduced to address\\nlimitations of CNNs when applied to inverse graphics\\ntasks, which aim to ﬁnd physical processes used to pro-\\nduce images of the world [124]. The recent develop-\\nment of capsule network based on dynamic routing al-\\ngorithm [125] demonstrates its ability to describe the hi-\\nerarchical pose relationships between object parts. This\\ndevelopment is employed as a component in a pipeline\\nfor detecting fabricated images and videos as demon-\\nstrated in Fig. 8. A dynamic routing algorithm is de-\\nployed to route the outputs of the three capsules to the\\noutput capsules through a number of iterations to sepa-\\nrate between fake and real images. The method is eval-\\nuated through four datasets covering a wide range of\\nforged image and video attacks. They include the well-\\nknown Idiap Research Institute replay-attack dataset\\n[126], the deepfake face swapping dataset created by\\nAfchar et al. [102], the facial reenactment FaceForen-\\nsics dataset [127], produced by the Face2Face method\\n[52], and the fully computer-generated image dataset\\ngenerated by Rahmouni et al. [128]. The proposed\\nmethod yields the best performance compared to its\\ncompeting methods in all of these datasets. This shows\\nthe potential of the capsule network in building a gen-\\neral detection system that can work e ﬀectively for vari-\\nous forged image and video attacks.\\nShallow classiﬁers. Deepfake detection methods\\nmostly rely on the artifacts or inconsistency of intrinsic\\nfeatures between fake and real images or videos. Yang\\net al. [121] proposed a detection method by observing\\nthe di ﬀerences between 3D head poses comprising head\\n9', metadata={'source': 'data/1909.11573.pdf', 'page': 8}),\n"," Document(page_content='Fig. 8. Capsule network takes features obtained from the VGG-19\\nnetwork [119] to distinguish fake images or videos from the real ones\\n(top). The pre-processing step detects face region and scales it to the\\nsize of 128x128 before VGG-19 is used to extract latent features for\\nthe capsule network, which comprises three primary capsules and two\\noutput capsules, one for real and one for fake images (bottom). The\\nstatistical pooling constitutes an important part of capsule network\\nthat deals with forgery detection [123].\\norientation and position, which are estimated based on\\n68 facial landmarks of the central face region. The 3D\\nhead poses are examined because there is a shortcoming\\nin the deepfake face generation pipeline. The extracted\\nfeatures are fed into an SVM classiﬁer to obtain the\\ndetection results. Experiments on two datasets show the\\ngreat performance of the proposed approach against its\\ncompeting methods. The ﬁrst dataset, namely UADFV ,\\nconsists of 49 deep fake videos and their respective\\nreal videos [121]. The second dataset comprises 241\\nreal images and 252 deep fake images, which is a\\nsubset of data used in the DARPA MediFor GAN\\nImage /Video Challenge [129]. Likewise, a method to\\nexploit artifacts of deepfakes and face manipulations\\nbased on visual features of eyes, teeth and facial\\ncontours was studied in [130]. The visual artifacts arise\\nfrom lacking global consistency, wrong or imprecise\\nestimation of the incident illumination, or imprecise\\nestimation of the underlying geometry. For deepfakes\\ndetection, missing reﬂections and missing details in\\nthe eye and teeth areas are exploited as well as texture\\nfeatures extracted from the facial region based on facial\\nlandmarks. Accordingly, the eye feature vector, teeth\\nfeature vector and features extracted from the full-face\\ncrop are used. After extracting the features, two\\nclassiﬁers including logistic regression and small neural\\nnetwork are employed to classify the deepfakes from\\nreal videos. Experiments carried out on a video dataset\\ndownloaded from YouTube show the best result of\\n0.851 in terms of the area under the receiver operating\\ncharacteristics curve. The proposed method however\\nhas a disadvantage that requires images meeting certain\\nprerequisite such as open eyes or visual teeth.The use of photo response non uniformity (PRNU)\\nanalysis was proposed in [131] to detect deepfakes from\\nauthentic ones. PRNU is a component of sensor pattern\\nnoise, which is attributed to the manufacturing imper-\\nfection of silicon wafers and the inconsistent sensitivity\\nof pixels to light because of the variation of the physical\\ncharacteristics of the silicon wafers. The PRNU anal-\\nysis is widely used in image forensics [132–136] and\\nadvocated to use in [131] because the swapped face is\\nsupposed to alter the local PRNU pattern in the facial\\narea of video frames. The videos are converted into\\nframes, which are cropped to the questioned facial re-\\ngion. The cropped frames are then separated sequen-\\ntially into eight groups where an average PRNU pattern\\nis computed for each group. Normalised cross correla-\\ntion scores are calculated for comparisons of PRNU pat-\\nterns among these groups. A test dataset was created,\\nconsisting of 10 authentic videos and 16 manipulated\\nvideos, where the fake videos were produced from the\\ngenuine ones by the DeepFaceLab tool [38]. The anal-\\nysis shows a signiﬁcant statistical di ﬀerence in terms\\nof mean normalised cross correlation scores between\\ndeepfakes and the genuine. This analysis therefore sug-\\ngests that PRNU has a potential in deepfake detection\\nalthough a larger dataset would need to be tested.\\nWhen seeing a video or image with suspicion, users\\nnormally want to search for its origin. However, there is\\ncurrently no feasibility for such a tool. Hasan and Salah\\n[137] proposed the use of blockchain and smart con-\\ntracts to help users detect deepfake videos based on the\\nassumption that videos are only real when their sources\\nare traceable. Each video is associated with a smart con-\\ntract that links to its parent video and each parent video\\nhas a link to its child in a hierarchical structure. Through\\nthis chain, users can credibly trace back to the origi-\\nnal smart contract associated with pristine video even if\\nthe video has been copied multiple times. An impor-\\ntant attribute of the smart contract is the unique hashes\\nof the interplanetary ﬁle system, which is used to store\\nvideo and its metadata in a decentralized and content-\\naddressable manner [138]. The smart contract’s key fea-\\ntures and functionalities are tested against several com-\\nmon security challenges such as distributed denial of\\nservices, replay and man in the middle attacks to ensure\\nthe solution meeting security requirements. This ap-\\nproach is generic, and it can be extended to other types\\nof digital content, e.g., images, audios and manuscripts.\\n4. Discussions and Future Research Directions\\nWith the support of deep learning, deepfakes can be\\ncreated easier than ever before. The spread of these fake\\n10', metadata={'source': 'data/1909.11573.pdf', 'page': 9}),\n"," Document(page_content='Table 2: Summary of prominent deepfake detection methods\\nMethods Classiﬁers /\\nTechniquesKey Features Dealing\\nwithDatasets Used\\nEye blinking\\n[114]LRCN - Use LRCN to learn the temporal patterns of eye blink-\\ning.\\n- Based on the observation that blinking frequency of\\ndeepfakes is much smaller than normal.Videos Consist of 49 interview and presentation videos, and\\ntheir corresponding generated deepfakes.\\nIntra-frame and\\ntemporal in-\\nconsistencies\\n[112]CNN and\\nLSTMCNN is employed to extract frame-level features, which\\nare distributed to LSTM to construct sequence descrip-\\ntor useful for classiﬁcation.Videos A collection of 600 videos obtained from multiple\\nwebsites.\\nUsing face warp-\\ning artifacts [120]VGG16 [119],\\nResNet mod-\\nels [118]Artifacts are discovered using CNN models based on\\nresolution inconsistency between the warped face area\\nand the surrounding context.Videos - UADFV [121], containing 49 real videos and 49 fake\\nvideos with 32752 frames in total.\\n- DeepfakeTIMIT [69]\\nMesoNet [102] CNN - Two deep networks, i.e. Meso-4 and MesoInception-4\\nare introduced to examine deepfake videos at the meso-\\nscopic analysis level.\\n- Accuracy obtained on deepfake and FaceForensics\\ndatasets are 98% and 95% respectively.Videos Two datasets: deepfake one constituted from on-\\nline videos and the FaceForensics one created by the\\nFace2Face approach [52].\\nEye, teach and fa-\\ncial texture [130]Logistic re-\\ngression and\\nneural network\\n(NN)- Exploit facial texture di ﬀerences, and missing reﬂec-\\ntions and details in eye and teeth areas of deepfakes.\\n- Logistic regression and NN are used for classifying.Videos A video dataset downloaded from YouTube.\\nSpatio-temporal\\nfeatures with\\nRCN [103]RCN Temporal discrepancies across frames are explored\\nusing RCN that integrates convolutional network\\nDenseNet [61] and the gated recurrent unit cells [111]Videos FaceForensics ++ dataset, including 1,000 videos\\n[105].\\nSpatio-temporal\\nfeatures with\\nLSTM [139]Convolutional\\nbidirectional\\nrecurrent\\nLSTM net-\\nwork- An XceptionNet CNN is used for facial feature extrac-\\ntion while audio embeddings are obtained by stacking\\nmultiple convolution modules.\\n- Two loss functions, i.e. cross-entropy and Kullback-\\nLeibler divergence, are used.Videos FaceForensics ++[105] and Celeb-DF (5,639 deep-\\nfake videos) [107] datasets and the ASVSpoof 2019\\nLogical Access audio dataset [140].\\nAnalysis of\\nPRNU [131]PRNU - Analysis of noise patterns of light sensitive sensors of\\ndigital cameras due to their factory defects.\\n- Explore the di ﬀerences of PRNU patterns between the\\nauthentic and deepfake videos because face swapping is\\nbelieved to alter the local PRNU patterns.Videos Created by the authors, including 10 authentic and 16\\ndeepfake videos using DeepFaceLab [38].\\nPhoneme-viseme\\nmismatches [141]CNN - Exploit the mismatches between the dynamics of the\\nmouth shape, i.e. visemes, with a spoken phoneme.\\n- Focus on sounds associated with the M, B and\\nP phonemes as they require complete mouth closure\\nwhile deepfakes often incorrectly synthesize it.Videos Four in-the-wild lip-sync deepfakes from Instagram\\nand YouTube (www.instagram.com /billposters uk\\nand youtu.be /VWMEDacz3L4) and others are cre-\\nated using synthesis techniques, i.e. Audio-to-Video\\n(A2V) [68] and Text-to-Video (T2V) [142].\\nUsing attribution-\\nbased conﬁdence\\n(ABC) metric\\n[143]ResNet50\\nmodel [118],\\npre-trained on\\nVGGFace2\\n[144]- The ABC metric [145] is used to detect deepfake\\nvideos without accessing to training data.\\n- ABC values obtained for original videos are greater\\nthan 0.94 while those of deepfakes have low ABC val-\\nues.Videos VidTIMIT and two other original\\ndatasets obtained from the COHFACE\\n(https: //www.idiap.ch /dataset /cohface) and from\\nYouTube. datasets from COHFACE [146] and\\nYouTube are used to generate two deepfake datasets\\nby commercial website https: //deepfakesweb.com\\nand another deepfake dataset is DeepfakeTIMIT\\n[147].\\nUsing appearance\\nand behaviour\\n[148]Rules based on\\nfacial and be-\\nhavioural fea-\\ntures.Temporal, behavioral biometric based on facial expres-\\nsions and head movements are learned using ResNet-\\n101 [118] while static facial biometric is obtained using\\nVGG [65].Videos The world leaders dataset [1], FaceForensics ++\\n[105], Google /Jigsaw deepfake detection dataset\\n[106], DFDC [109] and Celeb-DF [107].\\nFakeCatcher\\n[149]CNN Extract biological signals in portrait videos and use\\nthem as an implicit descriptor of authenticity because\\nthey are not spatially and temporally well-preserved in\\ndeepfakes.Videos UADFV [121], FaceForensics [127], FaceForen-\\nsics++[105], Celeb-DF [107], and a new dataset of\\n142 videos, independent of the generative model, res-\\nolution, compression, content, and context.\\nEmotion audio-\\nvisual a ﬀective\\ncues [150]Siamese\\nnetwork [86]Modality and emotion embedding vectors for the face\\nand speech are extracted for deepfake detection.Videos DeepfakeTIMIT [147] and DFDC [109].\\nHead poses [121] SVM - Features are extracted using 68 landmarks of the face\\nregion.\\n- Use SVM to classify using the extracted features.Videos /\\nImages- UADFV consists of 49 deep fake videos and their\\nrespective real videos.\\n- 241 real images and 252 deep fake images from\\nDARPA MediFor GAN Image /Video Challenge.\\nCapsule-forensics\\n[123]Capsule net-\\nworks- Latent features extracted by VGG-19 network [119]\\nare fed into the capsule network for classiﬁcation.\\n- A dynamic routing algorithm [125] is used to route\\nthe outputs of three convolutional capsules to two out-\\nput capsules, one for fake and another for real images,\\nthrough a number of iterations.Videos /\\nImagesFour datasets: the Idiap Research Institute replay-\\nattack [126], deepfake face swapping by [102], facial\\nreenactment FaceForensics [127], and fully computer-\\ngenerated image set using [128].\\n11', metadata={'source': 'data/1909.11573.pdf', 'page': 10}),\n"," Document(page_content='Methods Classiﬁers /\\nTechniquesKey Features Dealing\\nwithDatasets Used\\nPreprocessing\\ncombined with\\ndeep network\\n[74]DCGAN,\\nWGAN-GP and\\nPGGAN.- Enhance generalization ability of deep learning mod-\\nels to detect GAN generated images.\\n- Remove low level features of fake images.\\n- Force deep networks to focus more on pixel level sim-\\nilarity between fake and real images to improve gener-\\nalization ability.Images - Real dataset: CelebA-HQ [62], including high qual-\\nity face images of 1024x1024 resolution.\\n- Fake datasets: generated by DCGAN [88], WGAN-\\nGP [90] and PGGAN [62].\\nAnalyzing con-\\nvolutional traces\\n[151]KNN, SVM, and\\nlinear discrim-\\ninant analysis\\n(LDA)Using expectation-maximization algorithm to extract\\nlocal features pertaining to convolutional generative\\nprocess of GAN-based image deepfake generators.Images Authentic images from CelebA and correspond-\\ning deepfakes are created by ﬁve di ﬀerent GANs\\n(group-wise deep whitening-and-coloring transfor-\\nmation GDWCT [152], StarGAN [153], AttGAN\\n[154], StyleGAN [51], StyleGAN2 [155]).\\nBag of words\\nand shallow\\nclassiﬁers [78]SVM, RF, MLP Extract discriminant features using bag of words\\nmethod and feed these features into SVM, RF and MLP\\nfor binary classiﬁcation: innocent vs fabricated.Images The well-known LFW face database [156], containing\\n13,223 images with resolution of 250x250.\\nPairwise learn-\\ning [85]CNN con-\\ncatenated to\\nCFFNTwo-phase procedure: feature extraction using CFFN\\nbased on the Siamese network architecture [86] and\\nclassiﬁcation using CNN.Images - Face images: real ones from CelebA [87], and\\nfake ones generated by DCGAN [88], WGAN [89],\\nWGAN-GP [90], least squares GAN [91], and PG-\\nGAN [62].\\n- General images: real ones from ILSVRC12 [92],\\nand fake ones generated by BIGGAN [93], self-\\nattention GAN [94] and spectral normalization GAN\\n[95].\\nDefenses against\\nadversarial per-\\nturbations in\\ndeepfakes [157]VGG [65] and\\nResNet [118]- Introduce adversarial perturbations to enhance deep-\\nfakes and fool deepfake detectors.\\n- Improve accuracy of deepfake detectors using Lips-\\nchitz regularization and deep image prior techniques.Images 5,000 real images from CelebA [87] and 5,000 fake\\nimages created by the “Few-Shot Face Translation\\nGAN” method [158].\\nFace X-ray\\n[159]CNN - Try to locate the blending boundary between the target\\nand original faces instead of capturing the synthesized\\nartifacts of speciﬁc manipulations.\\n- Can be trained without fake images.Images FaceForensics ++[105], DeepfakeDetection (DFD)\\n[106], DFDC [109] and Celeb-DF [107].\\nUsing common\\nartifacts of\\nCNN-generated\\nimages [160]ResNet-50 [118]\\npre-trained with\\nImageNet [92]Train the classiﬁer using a large number of fake images\\ngenerated by a high-performing unconditional GAN\\nmodel, i.e., PGGAN [62] and evaluate how well the\\nclassiﬁer generalizes to other CNN-synthesized images.Images A new dataset of CNN-generated images, namely\\nForenSynths, consisting of synthesized images from\\n11 models such as StyleGAN [51], super-resolution\\nmethods [161] and FaceForensics ++[105].\\nUsing convolu-\\ntional traces on\\nGAN-based im-\\nages [162]KNN, SVM, and\\nLDATraining the expectation-maximization algorithm [163]\\nto detect and extract discriminative features via a ﬁn-\\ngerprint that represents the convolutional traces left by\\nGANs during image generation.Images A dataset of images generated by ten GAN models,\\nincluding CycleGAN [164], StarGAN [153], AttGAN\\n[154], GDWCT [152], StyleGAN [51], StyleGAN2\\n[155], PGGAN [62], FaceForensics ++[105], IMLE\\n[165], and SPADE [42].\\nUsing deep fea-\\ntures extracted\\nby CNN [100]A new CNN\\nmodel, namely\\nSCnetThe CNN-based SCnet is able to automatically learn\\nhigh-level forensics features of image data thanks to a\\nhierarchical feature extraction block, which is formed\\nby stacking four convolutional layers.Images A dataset of 321,378 face images, created by apply-\\ning the Glow model [101] to the CelebA face image\\ndataset [87].\\ncontents is also quicker thanks to the development of\\nsocial media platforms [166]. Sometimes deepfakes do\\nnot need to be spread to massive audience to cause detri-\\nmental e ﬀects. People who create deepfakes with ma-\\nlicious purpose only need to deliver them to target au-\\ndiences as part of their sabotage strategy without using\\nsocial media. For example, this approach can be utilized\\nby intelligence services trying to inﬂuence decisions\\nmade by important people such as politicians, leading to\\nnational and international security threats [167]. Catch-\\ning the deepfake alarming problem, research commu-\\nnity has focused on developing deepfake detection algo-\\nrithms and numerous results have been reported. This\\npaper has reviewed the state-of-the-art methods and a\\nsummary of typical approaches is provided in Table 2.\\nIt is noticeable that a battle between those who use ad-\\nvanced machine learning to create deepfakes with those\\nwho make e ﬀort to detect deepfakes is growing.Deepfakes’ quality has been increasing and the per-\\nformance of detection methods needs to be improved\\naccordingly. The inspiration is that what AI has broken\\ncan be ﬁxed by AI as well [168]. Detection methods are\\nstill in their early stage and various methods have been\\nproposed and evaluated but using fragmented datasets.\\nAn approach to improve performance of detection meth-\\nods is to create a growing updated benchmark dataset of\\ndeepfakes to validate the ongoing development of detec-\\ntion methods. This will facilitate the training process of\\ndetection models, especially those based on deep learn-\\ning, which requires a large training set [108].\\nImproving performance of deepfake detection meth-\\nods is important, especially in cross-forgery and cross-\\ndataset scenarios. Most detection models are designed\\nand evaluated in the same-forgery and in-dataset exper-\\niments, which do not ensure their generalization capa-\\nbility. Some previous studies have addressed this issue,\\n12', metadata={'source': 'data/1909.11573.pdf', 'page': 11}),\n"," Document(page_content='e.g., in [104, 116, 160, 169, 170], but more work needs\\nto be done in this direction. A model trained on a spe-\\nciﬁc forgery needs to be able to work against another\\nunknown one because potential deepfake types are not\\nnormally known in the real-world scenarios. Likewise,\\ncurrent detection methods mostly focus on drawbacks\\nof the deepfake generation pipelines, i.e., ﬁnding weak-\\nness of the competitors to attack them. This kind of\\ninformation and knowledge is not always available in\\nadversarial environments where attackers commonly at-\\ntempt not to reveal such deepfake creation technologies.\\nRecent works on adversarial perturbation attacks to fool\\nDNN-based detectors make the deepfake detection task\\nmore di ﬃcult [157, 171–174]. These are real challenges\\nfor detection method development and a future study\\nneeds to focus on introducing more robust, scalable and\\ngeneralizable methods.\\nAnother research direction is to integrate detection\\nmethods into distribution platforms such as social me-\\ndia to increase its e ﬀectiveness in dealing with the\\nwidespread impact of deepfakes. The screening or ﬁl-\\ntering mechanism using e ﬀective detection methods can\\nbe implemented on these platforms to ease the deep-\\nfakes detection [167]. Legal requirements can be made\\nfor tech companies who own these platforms to remove\\ndeepfakes quickly to reduce its impacts. In addition,\\nwatermarking tools can also be integrated into devices\\nthat people use to make digital contents to create im-\\nmutable metadata for storing originality details such as\\ntime and location of multimedia contents as well as\\ntheir untampered attestment [167]. This integration is\\ndiﬃcult to implement but a solution for this could be\\nthe use of the disruptive blockchain technology. The\\nblockchain has been used e ﬀectively in many areas and\\nthere are very few studies so far addressing the deep-\\nfake detection problems based on this technology. As\\nit can create a chain of unique unchangeable blocks of\\nmetadata, it is a great tool for digital provenance solu-\\ntion. The integration of blockchain technologies to this\\nproblem has demonstrated certain results [137] but this\\nresearch direction is far from mature.\\nUsing detection methods to spot deepfakes is crucial,\\nbut understanding the real intent of people publishing\\ndeepfakes is even more important. This requires the\\njudgement of users based on social context in which\\ndeepfake is discovered, e.g. who distributed it and what\\nthey said about it [175]. This is critical as deepfakes\\nare getting more and more photorealistic and it is highly\\nanticipated that detection software will be lagging be-\\nhind deepfake creation technology. A study on social\\ncontext of deepfakes to assist users in such judgement\\nis thus worth performing.Videos and photographics have been widely used as\\nevidences in police investigation and justice cases. They\\nmay be introduced as evidences in a court of law by\\ndigital media forensics experts who have background in\\ncomputer or law enforcement and experience in collect-\\ning, examining and analysing digital information. The\\ndevelopment of machine learning and AI technologies\\nmight have been used to modify these digital contents\\nand thus the experts’ opinions may not be enough to au-\\nthenticate these evidences because even experts are un-\\nable to discern manipulated contents. This aspect needs\\nto take into account in courtrooms nowadays when im-\\nages and videos are used as evidences to convict per-\\npetrators because of the existence of a wide range of\\ndigital manipulation methods [176]. The digital me-\\ndia forensics results therefore must be proved to be\\nvalid and reliable before they can be used in courts.\\nThis requires careful documentation for each step of the\\nforensics process and how the results are reached. Ma-\\nchine learning and AI algorithms can be used to sup-\\nport the determination of the authenticity of digital me-\\ndia and have obtained accurate and reliable results, e.g.,\\n[177, 178], but most of these algorithms are unexplain-\\nable. This creates a huge hurdle for the applications of\\nAI in forensics problems because not only the foren-\\nsics experts oftentimes do not have expertise in com-\\nputer algorithms, but the computer professionals also\\ncannot explain the results properly as most of these al-\\ngorithms are black box models [179]. This is more crit-\\nical as the most recent models with the most accurate\\nresults are based on deep learning methods consisting\\nof many neural network parameters. Researchers have\\nrecently attempted to create white box and explainable\\ndetection methods. An example is the approach pro-\\nposed by Giudice et al. [180] in which they use discrete\\ncosine transform statistics to detect so-called speciﬁc\\nGAN frequencies to di ﬀerentiate between real images\\nand deepfakes. Through the analysis of particular fre-\\nquency statistics, that method can be used to mathemat-\\nically explain whether a multimedia content is a deep-\\nfake and why it is. More research must be conducted in\\nthis area and explainable AI in computer vision there-\\nfore is a research direction that is needed to promote and\\nutilize the advances and advantages of AI and machine\\nlearning in digital media forensics.\\n5. Conclusions\\nDeepfakes have begun to erode trust of people in me-\\ndia contents as seeing them is no longer commensurate\\nwith believing in them. They could cause distress and\\n13', metadata={'source': 'data/1909.11573.pdf', 'page': 12}),\n"," Document(page_content='negative e ﬀects to those targeted, heighten disinforma-\\ntion and hate speech, and even could stimulate political\\ntension, inﬂame the public, violence or war. This is es-\\npecially critical nowadays as the technologies for creat-\\ning deepfakes are increasingly approachable and social\\nmedia platforms can spread those fake contents quickly.\\nThis survey provides a timely overview of deepfake cre-\\nation and detection methods and presents a broad dis-\\ncussion on challenges, potential trends, and future di-\\nrections in this area. This study therefore will be valu-\\nable for the artiﬁcial intelligence research community to\\ndevelop e ﬀective methods for tackling deepfakes.\\nDeclaration of Competing Interest\\nAuthors declare no conﬂict of interest.\\nReferences\\n[1] Shruti Agarwal, Hany Farid, Yuming Gu, Mingming He, Koki\\nNagano, and Hao Li. Protecting world leaders against deep\\nfakes. In Computer Vision and Pattern Recognition Workshops ,\\nvolume 1, pages 38–45, 2019.\\n[2] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-\\nAntoine Manzagol. Extracting and composing robust features\\nwith denoising autoencoders. In Proceedings of the 25th Inter-\\nnational Conference on Machine learning , pages 1096–1103,\\n2008.\\n[3] Diederik P Kingma and Max Welling. Auto-encoding varia-\\ntional Bayes. arXiv preprint arXiv:1312.6114 , 2013.\\n[4] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\\nYoshua Bengio. Generative adversarial nets. Advances in Neu-\\nral Information Processing Systems , 27:2672–2680, 2014.\\n[5] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Good-\\nfellow, and Brendan Frey. Adversarial autoencoders. arXiv\\npreprint arXiv:1511.05644 , 2015.\\n[6] Ayush Tewari, Michael Zollhoefer, Florian Bernard, Pablo\\nGarrido, Hyeongwoo Kim, Patrick Perez, and Christian\\nTheobalt. High-ﬁdelity monocular face reconstruction based\\non an unsupervised model-based face autoencoder. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence , 42\\n(2):357–370, 2018.\\n[7] Jiacheng Lin, Yang Li, and Guanci Yang. FPGAN: Face de-\\nidentiﬁcation method with generative adversarial networks for\\nsocial robots. Neural Networks , 133:132–147, 2021.\\n[8] Ming-Yu Liu, Xun Huang, Jiahui Yu, Ting-Chun Wang, and\\nArun Mallya. Generative adversarial networks for image and\\nvideo synthesis: Algorithms and applications. Proceedings of\\nthe IEEE , 109(5):839–862, 2021.\\n[9] Siwei Lyu. Detecting ’deepfake’ videos in the blink of an eye.\\nhttp://theconversation.com/detecting-deepfake-\\nvideos- in- the- blink- of- an- eye- 101072 , August\\n2018.\\n[10] Bloomberg. How faking videos became easy and why that’s\\nso scary. https://fortune.com/2018/09/11/deep-\\nfakes-obama-video/ , September 2018.\\n[11] Robert Chesney and Danielle Citron. Deepfakes and the new\\ndisinformation war: The coming age of post-truth geopolitics.\\nForeign A ﬀairs, 98:147, 2019.[12] T. Hwang. Deepfakes: A grounded threat assessment. Tech-\\nnical report, Centre for Security and Emerging Technologies,\\nGeorgetown University, 2020.\\n[13] Xinyi Zhou and Reza Zafarani. A survey of fake news: Fun-\\ndamental theories, detection methods, and opportunities. ACM\\nComputing Surveys (CSUR) , 53(5):1–40, 2020.\\n[14] Rohit Kumar Kaliyar, Anurag Goswami, and Pratik Narang.\\nDeepfake: improving fake news detection using tensor\\ndecomposition-based deep neural network. The Journal of Su-\\npercomputing , 77(2):1015–1037, 2021.\\n[15] Bin Guo, Yasan Ding, Lina Yao, Yunji Liang, and Zhiwen Yu.\\nThe future of false information detection on social media: New\\nperspectives and trends. ACM Computing Surveys (CSUR) , 53\\n(4):1–36, 2020.\\n[16] Patrick Tucker. The newest AI-enabled weapon: ‘deep-faking’\\nphotos of the earth. https://www.defenseone.com/\\ntechnology/2019/03/next-phase-ai-deep-faking-\\nwhole-world-and-china-ahead/155944/ , March 2019.\\n[17] T Fish. Deep fakes: AI-manipulated media will be\\n‘weaponised’ to trick military. https://www.express.\\nco . uk / news / science / 1109783 / deep - fakes -\\nai - artificial - intelligence - photos - video -\\nweaponised-china , April 2019.\\n[18] B Marr. The best (and scariest) examples of AI-enabled deep-\\nfakes. https://www.forbes.com/sites/bernardmarr/\\n2019/07/22/the-best-and-scariest-examples-of-\\nai-enabled-deepfakes/ , July 2019.\\n[19] Yisroel Mirsky and Wenke Lee. The creation and detection of\\ndeepfakes: A survey. ACM Computing Surveys (CSUR) , 54(1):\\n1–41, 2021.\\n[20] Luisa Verdoliva. Media forensics and deepfakes: an overview.\\nIEEE Journal of Selected Topics in Signal Processing , 14(5):\\n910–932, 2020.\\n[21] Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Vic-\\ntor Lempitsky. Few-shot adversarial learning of realistic neural\\ntalking head models. In Proceedings of the IEEE /CVF Inter-\\nnational Conference on Computer Vision , pages 9459–9468,\\n2019.\\n[22] J Damiani. A voice deepfake was used to scam a ceo\\nout of $243,000. https://www.forbes.com/sites/\\njessedamiani/2019/09/03/a-voice-deepfake-was-\\nused- to- scam- a- ceo- out- of- 243000/ , September\\n2019.\\n[23] S Samuel. A guy made a deepfake app to turn photos of women\\ninto nudes. it didn’t go well. https://www.vox.com/2019/\\n6/27/18761639/ai- deepfake- deepnude- app- nude-\\nwomen-porn , June 2019.\\n[24] The Guardian. Chinese deepfake app Zao sparks privacy\\nrow after going viral. https://www.theguardian.com/\\ntechnology/2019/sep/02/chinese-face-swap-app-\\nzao-triggers-privacy-fears-viral , September 2019.\\n[25] Siwei Lyu. Deepfake detection: Current challenges and next\\nsteps. In IEEE International Conference on Multimedia &\\nExpo Workshops (ICMEW) , pages 1–6. IEEE, 2020.\\n[26] Luca Guarnera, Oliver Giudice, Cristina Nastasi, and Sebas-\\ntiano Battiato. Preliminary forensics analysis of deepfake im-\\nages. In AEIT International Annual Conference (AEIT) , pages\\n1–6. IEEE, 2020.\\n[27] Mousa Tayseer Jafar, Mohammad Ababneh, Mohammad Al-\\nZoube, and Ammar Elhassan. Forensics and analysis of deep-\\nfake videos. In The 11th International Conference on Infor-\\nmation and Communication Systems (ICICS) , pages 053–058.\\nIEEE, 2020.\\n[28] Loc Trinh, Michael Tsang, Sirisha Rambhatla, and Yan Liu.\\nInterpretable and trustworthy deepfake detection via dynamic\\n14', metadata={'source': 'data/1909.11573.pdf', 'page': 13}),\n"," Document(page_content='prototypes. In Proceedings of the IEEE /CVF Winter Confer-\\nence on Applications of Computer Vision , pages 1973–1983,\\n2021.\\n[29] Mohammed Akram Younus and Taha Mohammed Hasan. Ef-\\nfective and fast deepfake detection method based on haar\\nwavelet transform. In International Conference on Computer\\nScience and Software Engineering (CSASE) , pages 186–190.\\nIEEE, 2020.\\n[30] M Turek. Media Forensics (MediFor). https://www.darpa.\\nmil/program/media-forensics , January 2019.\\n[31] M Schroepfer. Creating a data set and a challenge for deep-\\nfakes. https://ai.facebook.com/blog/deepfake-\\ndetection-challenge , September 2019.\\n[32] Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez,\\nAythami Morales, and Javier Ortega-Garcia. Deepfakes and\\nbeyond: A survey of face manipulation and fake detection. In-\\nformation Fusion , 64:131–148, 2020.\\n[33] Abhijith Punnappurath and Michael S Brown. Learning raw\\nimage reconstruction-aware deep image compressors. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence , 42\\n(4):1013–1019, 2019.\\n[34] Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro\\nKatto. Energy compaction-based image compression using\\nconvolutional autoencoder. IEEE Transactions on Multimedia ,\\n22(4):860–873, 2019.\\n[35] Jan Chorowski, Ron J Weiss, Samy Bengio, and A ¨aron van den\\nOord. Unsupervised speech representation learning using\\nWaveNet autoencoders. IEEE /ACM Transactions on Audio,\\nSpeech, and Language Processing , 27(12):2041–2053, 2019.\\n[36] Faceswap: Deepfakes software for all. https://github.\\ncom/deepfakes/faceswap .\\n[37] FakeApp 2.2.0. https://www.malavida.com/en/soft/\\nfakeapp/ .\\n[38] DeepFaceLab. https : / / github . com / iperov /\\nDeepFaceLab , .\\n[39] DFaker. https://github.com/dfaker/df .\\n[40] DeepFake tf: Deepfake based on tensorﬂow. https://\\ngithub.com/StromWine/DeepFake_tf , .\\n[41] Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo\\nAila, Jaakko Lehtinen, and Jan Kautz. Few-shot unsupervised\\nimage-to-image translation. In Proceedings of the IEEE /CVF\\nInternational Conference on Computer Vision , pages 10551–\\n10560, 2019.\\n[42] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan\\nZhu. Semantic image synthesis with spatially-adaptive nor-\\nmalization. In Proceedings of the IEEE /CVF Conference on\\nComputer Vision and Pattern Recognition , pages 2337–2346,\\n2019.\\n[43] DeepFaceLab: Explained and usage tutorial. https :\\n/ /mrdeepfakes. com/ forums/ thread- deepfacelab-\\nexplained-and-usage-tutorial .\\n[44] DSSIM. https://github.com/keras- team/keras-\\ncontrib / blob / master / keras _ contrib / losses /\\ndssim.py .\\n[45] Alexandros Lattas, Stylianos Moschoglou, Baris Gecer,\\nStylianos Ploumpis, Vasileios Triantafyllou, Abhijeet Ghosh,\\nand Stefanos Zafeiriou. AvatarMe: Realistically renderable\\n3D facial reconstruction “in-the-wild”. In Proceedings of the\\nIEEE /CVF Conference on Computer Vision and Pattern Recog-\\nnition , pages 760–769, 2020.\\n[46] Sungjoo Ha, Martin Kersner, Beomsu Kim, Seokjun Seo, and\\nDongyoung Kim. Marionette: Few-shot face reenactment pre-\\nserving identity of unseen targets. In Proceedings of the AAAI\\nConference on Artiﬁcial Intelligence , volume 34, pages 10893–\\n10900, 2020.[47] Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin\\nTong. Disentangled and controllable face image generation\\nvia 3D imitative-contrastive learning. In Proceedings of the\\nIEEE /CVF Conference on Computer Vision and Pattern Recog-\\nnition , pages 5154–5163, 2020.\\n[48] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian\\nBernard, Hans-Peter Seidel, Patrick P ´erez, Michael Zollhofer,\\nand Christian Theobalt. StyleRig: Rigging StyleGAN for 3D\\ncontrol over portrait images. In Proceedings of the IEEE /CVF\\nConference on Computer Vision and Pattern Recognition ,\\npages 6142–6151, 2020.\\n[49] Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang\\nWen. FaceShifter: Towards high ﬁdelity and occlusion aware\\nface swapping. arXiv preprint arXiv:1912.13457 , 2019.\\n[50] Yuval Nirkin, Yosi Keller, and Tal Hassner. FSGAN: Subject\\nagnostic face swapping and reenactment. In Proceedings of\\nthe IEEE /CVF International Conference on Computer Vision ,\\npages 7184–7193, 2019.\\n[51] Tero Karras, Samuli Laine, and Timo Aila. A style-based gen-\\nerator architecture for generative adversarial networks. In Pro-\\nceedings of the IEEE /CVF Conference on Computer Vision and\\nPattern Recognition , pages 4401–4410, 2019.\\n[52] Justus Thies, Michael Zollhofer, Marc Stamminger, Christian\\nTheobalt, and Matthias Nießner. Face2Face: Real-time face\\ncapture and reenactment of RGB videos. In Proceedings of\\nthe IEEE Conference on Computer Vision and Pattern Recog-\\nnition , pages 2387–2395, 2016.\\n[53] Justus Thies, Michael Zollh ¨ofer, and Matthias Nießner. De-\\nferred neural rendering: Image synthesis using neural textures.\\nACM Transactions on Graphics (TOG) , 38(4):1–12, 2019.\\n[54] Kyle Olszewski, Sergey Tulyakov, Oliver Woodford, Hao Li,\\nand Linjie Luo. Transformable bottleneck networks. In Pro-\\nceedings of the IEEE /CVF International Conference on Com-\\nputer Vision , pages 7648–7657, 2019.\\n[55] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A\\nEfros. Everybody dance now. In Proceedings of the IEEE /CVF\\nInternational Conference on Computer Vision , pages 5933–\\n5942, 2019.\\n[56] Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian\\nTheobalt, and Matthias Nießner. Neural voice puppetry:\\nAudio-driven facial reenactment. In European Conference on\\nComputer Vision , pages 716–731. Springer, 2020.\\n[57] Keras-VGGFace: VGGFace implementation with Keras\\nframework. https://github.com/rcmalli/keras-\\nvggface .\\n[58] Faceswap-GAN. https : / / github . com / shaoanlu /\\nfaceswap-GAN , .\\n[59] FaceNet. https : / / github . com / davidsandberg /\\nfacenet , .\\n[60] CycleGAN. https://github.com/junyanz/pytorch-\\nCycleGAN-and-pix2pix .\\n[61] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\\nian Q Weinberger. Densely connected convolutional networks.\\nInProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition , pages 4700–4708, 2017.\\n[62] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\\nProgressive growing of GANs for improved quality, stability,\\nand variation. arXiv preprint arXiv:1710.10196 , 2017.\\n[63] Pavel Korshunov and S ´ebastien Marcel. Vulnerability assess-\\nment and detection of deepfake videos. In 2019 International\\nConference on Biometrics (ICB) , pages 1–6. IEEE, 2019.\\n[64] VidTIMIT database. http://conradsanderson.id.au/\\nvidtimit/ .\\n[65] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman.\\nDeep face recognition. In Proceedings of the British Machine\\n15', metadata={'source': 'data/1909.11573.pdf', 'page': 14}),\n"," Document(page_content='Vision Conference (BMVC) , pages 41.1–41.12, 2015.\\n[66] Florian Schro ﬀ, Dmitry Kalenichenko, and James Philbin.\\nFaceNet: A uniﬁed embedding for face recognition and clus-\\ntering. In Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition , pages 815–823, 2015.\\n[67] Joon Son Chung, Andrew Senior, Oriol Vinyals, and An-\\ndrew Zisserman. Lip reading sentences in the wild. In 2017\\nIEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 3444–3453. IEEE, 2017.\\n[68] Supasorn Suwajanakorn, Steven M Seitz, and Ira\\nKemelmacher-Shlizerman. Synthesizing Obama: learn-\\ning lip sync from audio. ACM Transactions on Graphics\\n(ToG) , 36(4):1–13, 2017.\\n[69] Pavel Korshunov and S ´ebastien Marcel. Speaker inconsistency\\ndetection in tampered video. In The 26th European Signal\\nProcessing Conference (EUSIPCO) , pages 2375–2379. IEEE,\\n2018.\\n[70] Javier Galbally and S ´ebastien Marcel. Face anti-spooﬁng\\nbased on general image quality assessment. In The 22nd In-\\nternational Conference on Pattern Recognition , pages 1173–\\n1178. IEEE, 2014.\\n[71] Robert Chesney and Danielle Keats Citron. Deep fakes: A\\nlooming challenge for privacy, democracy, and national secu-\\nrity. Democracy, and National Security , 107, 2018.\\n[72] Oscar de Lima, Sean Franklin, Shreshtha Basu, Blake\\nKarwoski, and Annet George. Deepfake detection us-\\ning spatiotemporal convolutional networks. arXiv preprint\\narXiv:2006.14749 , 2020.\\n[73] Irene Amerini and Roberto Caldelli. Exploiting prediction er-\\nror inconsistencies through LSTM-based classiﬁers to detect\\ndeepfake videos. In Proceedings of the 2020 ACM Workshop\\non Information Hiding and Multimedia Security , pages 97–\\n102, 2020.\\n[74] Xinsheng Xuan, Bo Peng, Wei Wang, and Jing Dong. On\\nthe generalization of GAN image forensics. In Chinese Con-\\nference on Biometric Recognition , pages 134–141. Springer,\\n2019.\\n[75] Pengpeng Yang, Rongrong Ni, and Yao Zhao. Recapture image\\nforensics based on laplacian convolutional neural networks. In\\nInternational Workshop on Digital Watermarking , pages 119–\\n128. Springer, 2016.\\n[76] Belhassen Bayar and Matthew C Stamm. A deep learning ap-\\nproach to universal image manipulation detection using a new\\nconvolutional layer. In Proceedings of the 4th ACM Workshop\\non Information Hiding and Multimedia Security , pages 5–10,\\n2016.\\n[77] Yinlong Qian, Jing Dong, Wei Wang, and Tieniu Tan. Deep\\nlearning for steganalysis via convolutional neural networks. In\\nMedia Watermarking, Security, and Forensics , volume 9409,\\npage 94090J, 2015.\\n[78] Ying Zhang, Lilei Zheng, and Vrizlynn LL Thing. Automated\\nface swapping and its detection. In The 2nd International Con-\\nference on Signal and Image Processing (ICSIP) , pages 15–19.\\nIEEE, 2017.\\n[79] Xin Wang, Nicolas Thome, and Matthieu Cord. Gaze latent\\nsupport vector machine for image classiﬁcation improved by\\nweakly supervised region selection. Pattern Recognition , 72:\\n59–71, 2017.\\n[80] Shuang Bai. Growing random forest on deep convolutional\\nneural networks for scene categorization. Expert Systems with\\nApplications , 71:279–287, 2017.\\n[81] Lilei Zheng, Stefan Du ﬀner, Khalid Idrissi, Christophe Gar-\\ncia, and Atilla Baskurt. Siamese multi-layer perceptrons for\\ndimensionality reduction and face identiﬁcation. Multimedia\\nTools and Applications , 75(9):5055–5073, 2016.[82] Sakshi Agarwal and Lav R Varshney. Limits of deepfake\\ndetection: A robust estimation viewpoint. arXiv preprint\\narXiv:1905.03493 , 2019.\\n[83] Ueli M Maurer. Authentication theory and hypothesis testing.\\nIEEE Transactions on Information Theory , 46(4):1350–1356,\\n2000.\\n[84] Iryna Korshunova, Wenzhe Shi, Joni Dambre, and Lucas Theis.\\nFast face-swap using convolutional neural networks. In Pro-\\nceedings of the IEEE International Conference on Computer\\nVision , pages 3677–3685, 2017.\\n[85] Chih-Chung Hsu, Yi-Xiu Zhuang, and Chia-Yen Lee. Deep\\nfake image detection based on pairwise learning. Applied Sci-\\nences , 10(1):370, 2020.\\n[86] Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a\\nsimilarity metric discriminatively, with application to face ver-\\niﬁcation. In IEEE Computer Society Conference on Computer\\nVision and Pattern Recognition (CVPR’05) , volume 1, pages\\n539–546. IEEE, 2005.\\n[87] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep\\nlearning face attributes in the wild. In Proceedings of the IEEE\\nInternational Conference on Computer Vision , pages 3730–\\n3738, 2015.\\n[88] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-\\nvised representation learning with deep convolutional genera-\\ntive adversarial networks. arXiv preprint arXiv:1511.06434 ,\\n2015.\\n[89] Martin Arjovsky, Soumith Chintala, and L ´eon Bottou. Wasser-\\nstein generative adversarial networks. In International Confer-\\nence on Machine Learning , pages 214–223. PMLR, 2017.\\n[90] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Du-\\nmoulin, and Aaron Courville. Improved training of Wasser-\\nstein GANs. arXiv preprint arXiv:1704.00028 , 2017.\\n[91] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen\\nWang, and Stephen Paul Smolley. Least squares generative\\nadversarial networks. In Proceedings of the IEEE International\\nConference on Computer Vision , pages 2794–2802, 2017.\\n[92] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, et al. ImageNet large scale\\nvisual recognition challenge. International Journal of Com-\\nputer Vision , 115(3):211–252, 2015.\\n[93] Andrew Brock, Je ﬀDonahue, and Karen Simonyan. Large\\nscale GAN training for high ﬁdelity natural image synthesis.\\narXiv preprint arXiv:1809.11096 , 2018.\\n[94] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus-\\ntus Odena. Self-attention generative adversarial networks. In\\nInternational Conference on Machine Learning , pages 7354–\\n7363. PMLR, 2019.\\n[95] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and\\nYuichi Yoshida. Spectral normalization for generative adver-\\nsarial networks. arXiv preprint arXiv:1802.05957 , 2018.\\n[96] Hany Farid. Image forgery detection. IEEE Signal Processing\\nMagazine , 26(2):16–25, 2009.\\n[97] Huaxiao Mo, Bolin Chen, and Weiqi Luo. Fake faces iden-\\ntiﬁcation via convolutional neural network. In Proceedings of\\nthe 6th ACM Workshop on Information Hiding and Multimedia\\nSecurity , pages 43–47, 2018.\\n[98] Francesco Marra, Diego Gragnaniello, Davide Cozzolino, and\\nLuisa Verdoliva. Detection of GAN-generated fake images\\nover social networks. In 2018 IEEE Conference on Multimedia\\nInformation Processing and Retrieval (MIPR) , pages 384–389.\\nIEEE, 2018.\\n[99] Chih-Chung Hsu, Chia-Yen Lee, and Yi-Xiu Zhuang. Learning\\nto detect fake face images in the wild. In 2018 International\\nSymposium on Computer, Consumer and Control (IS3C) , pages\\n16', metadata={'source': 'data/1909.11573.pdf', 'page': 15}),\n"," Document(page_content='388–391. IEEE, 2018.\\n[100] Zhiqing Guo, Lipin Hu, Ming Xia, and Gaobo Yang. Blind\\ndetection of glow-based facial forgery. Multimedia Tools and\\nApplications , 80(5):7687–7710, 2021.\\n[101] Diederik P Kingma and Prafulla Dhariwal. Glow: genera-\\ntive ﬂow with invertible 1 ×1 convolutions. In Proceedings\\nof the 32nd International Conference on Neural Information\\nProcessing Systems , pages 10236–10245, 2018.\\n[102] Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao\\nEchizen. MesoNet: a compact facial video forgery detection\\nnetwork. In 2018 IEEE International Workshop on Information\\nForensics and Security (WIFS) , pages 1–7. IEEE, 2018.\\n[103] Ekraam Sabir, Jiaxin Cheng, Ayush Jaiswal, Wael AbdAl-\\nmageed, Iacopo Masi, and Prem Natarajan. Recurrent con-\\nvolutional strategies for face manipulation detection in videos.\\nProceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition Workshops , 3(1):80–87, 2019.\\n[104] Tianchen Zhao, Xiang Xu, Mingze Xu, Hui Ding, Yuanjun\\nXiong, and Wei Xia. Learning self-consistency for deepfake\\ndetection. In Proceedings of the IEEE /CVF International Con-\\nference on Computer Vision , pages 15023–15033, 2021.\\n[105] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian\\nRiess, Justus Thies, and Matthias Nießner. FaceForensics ++:\\nLearning to detect manipulated facial images. In Proceedings\\nof the IEEE /CVF International Conference on Computer Vi-\\nsion, pages 1–11, 2019.\\n[106] Nick Dufour and Andrew Gully. Contributing data to deepfake\\ndetection research. https://ai.googleblog.com/2019/\\n09 / contributing - data - to - deepfake - detection .\\nhtml , September 2019.\\n[107] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei\\nLyu. Celeb-DF: A large-scale challenging dataset for deep-\\nfake forensics. In Proceedings of the IEEE /CVF Conference on\\nComputer Vision and Pattern Recognition , pages 3207–3216,\\n2020.\\n[108] Brian Dolhansky, Joanna Bitton, Ben Pﬂaum, Jikuo Lu,\\nRuss Howes, Menglin Wang, and Cristian Canton Ferrer.\\nThe deepfake detection challenge dataset. arXiv preprint\\narXiv:2006.07397 , 2020.\\n[109] Brian Dolhansky, Russ Howes, Ben Pﬂaum, Nicole Baram,\\nand Cristian Canton Ferrer. The deepfake detection challenge\\n(DFDC) preview dataset. arXiv preprint arXiv:1910.08854 ,\\n2019.\\n[110] Liming Jiang, Ren Li, Wayne Wu, Chen Qian, and\\nChen Change Loy. DeeperForensics-1.0: A large-scale dataset\\nfor real-world face forgery detection. In Proceedings of the\\nIEEE /CVF Conference on Computer Vision and Pattern Recog-\\nnition , pages 2889–2898, 2020.\\n[111] Kyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gulcehre,\\nDzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\\nYoshua Bengio. Learning phrase representations using RNN\\nencoder-decoder for statistical machine translation. arXiv\\npreprint arXiv:1406.1078 , 2014.\\n[112] David G ¨uera and Edward J Delp. Deepfake video detection\\nusing recurrent neural networks. In 15th IEEE International\\nConference on Advanced Video and Signal based Surveillance\\n(AVSS) , pages 1–6. IEEE, 2018.\\n[113] Ivan Laptev, Marcin Marszalek, Cordelia Schmid, and Ben-\\njamin Rozenfeld. Learning realistic human actions from\\nmovies. In IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 1–8. IEEE, 2008.\\n[114] Yuezun Li, Ming-Ching Chang, and Siwei Lyu. In ictu oculi:\\nExposing ai created fake videos by detecting eye blinking. In\\n2018 IEEE International Workshop on Information Forensics\\nand Security (WIFS) , pages 1–7. IEEE, 2018.[115] Je ﬀrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,\\nMarcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and\\nTrevor Darrell. Long-term recurrent convolutional networks\\nfor visual recognition and description. In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recogni-\\ntion, pages 2625–2634, 2015.\\n[116] Roberto Caldelli, Leonardo Galteri, Irene Amerini, and Al-\\nberto Del Bimbo. Optical ﬂow based CNN for detection of\\nunlearnt deepfake manipulations. Pattern Recognition Letters ,\\n146:31–37, 2021.\\n[117] Irene Amerini, Leonardo Galteri, Roberto Caldelli, and Al-\\nberto Del Bimbo. Deepfake video detection through opti-\\ncal ﬂow based CNN. In Proceedings of the IEEE /CVF In-\\nternational Conference on Computer Vision Workshops , pages\\n1205–1207, 2019.\\n[118] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\nings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[119] Karen Simonyan and Andrew Zisserman. Very deep convo-\\nlutional networks for large-scale image recognition. arXiv\\npreprint arXiv:1409.1556 , 2014.\\n[120] Yuezun Li and Siwei Lyu. Exposing deepfake videos by detect-\\ning face warping artifacts. arXiv preprint arXiv:1811.00656 ,\\n2018.\\n[121] Xin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakes us-\\ning inconsistent head poses. In IEEE International Conference\\non Acoustics, Speech and Signal Processing (ICASSP) , pages\\n8261–8265. IEEE, 2019.\\n[122] Peng Zhou, Xintong Han, Vlad I Morariu, and Larry S Davis.\\nTwo-stream neural networks for tampered face detection. In\\nIEEE Conference on Computer Vision and Pattern Recognition\\nWorkshops (CVPRW) , pages 1831–1839. IEEE, 2017.\\n[123] Huy H Nguyen, Junichi Yamagishi, and Isao Echizen. Capsule-\\nforensics: Using capsule networks to detect forged images\\nand videos. In IEEE International Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP) , pages 2307–2311.\\nIEEE, 2019.\\n[124] Geo ﬀrey E Hinton, Alex Krizhevsky, and Sida D Wang. Trans-\\nforming auto-encoders. In International Conference on Artiﬁ-\\ncial Neural Networks , pages 44–51. Springer, 2011.\\n[125] Sara Sabour, Nicholas Frosst, and Geo ﬀrey E Hinton. Dynamic\\nrouting between capsules. In Proceedings of the 31st Interna-\\ntional Conference on Neural Information Processing Systems ,\\npages 3859–3869, 2017.\\n[126] Ivana Chingovska, Andr ´e Anjos, and S ´ebastien Marcel. On\\nthe eﬀectiveness of local binary patterns in face anti-spooﬁng.\\nInProceedings of the International Conference of Biometrics\\nSecial Interest Group (BIOSIG) , pages 1–7. IEEE, 2012.\\n[127] Andreas R ¨ossler, Davide Cozzolino, Luisa Verdoliva, Christian\\nRiess, Justus Thies, and Matthias Nießner. FaceForensics: A\\nlarge-scale video dataset for forgery detection in human faces.\\narXiv preprint arXiv:1803.09179 , 2018.\\n[128] Nicolas Rahmouni, Vincent Nozick, Junichi Yamagishi, and\\nIsao Echizen. Distinguishing computer graphics from natu-\\nral images using convolution neural networks. In IEEE Work-\\nshop on Information Forensics and Security (WIFS) , pages 1–\\n6. IEEE, 2017.\\n[129] Haiying Guan, Mark Kozak, Eric Robertson, Yooyoung Lee,\\nAmy N Yates, Andrew Delgado, Daniel Zhou, Timothee\\nKheyrkhah, Je ﬀSmith, and Jonathan Fiscus. MFC datasets:\\nLarge-scale benchmark datasets for media forensic challenge\\nevaluation. In IEEE Winter Applications of Computer Vision\\nWorkshops (WACVW) , pages 63–72. IEEE, 2019.\\n[130] Falko Matern, Christian Riess, and Marc Stamminger. Exploit-\\n17', metadata={'source': 'data/1909.11573.pdf', 'page': 16}),\n"," Document(page_content='ing visual artifacts to expose deepfakes and face manipulations.\\nInIEEE Winter Applications of Computer Vision Workshops\\n(WACVW) , pages 83–92. IEEE, 2019.\\n[131] Marissa Koopman, Andrea Macarulla Rodriguez, and Zeno\\nGeradts. Detection of deepfake video manipulation. In The\\n20th Irish Machine Vision and Image Processing Conference\\n(IMVIP) , pages 133–136, 2018.\\n[132] Kurt Rosenfeld and Husrev Taha Sencar. A study of the robust-\\nness of PRNU-based camera identiﬁcation. In Media Forensics\\nand Security , volume 7254, page 72540M, 2009.\\n[133] Chang-Tsun Li and Yue Li. Color-decoupled photo response\\nnon-uniformity for digital image forensics. IEEE Transactions\\non Circuits and Systems for Video Technology , 22(2):260–271,\\n2011.\\n[134] Xufeng Lin and Chang-Tsun Li. Large-scale image clustering\\nbased on camera ﬁngerprints. IEEE Transactions on Informa-\\ntion Forensics and Security , 12(4):793–808, 2016.\\n[135] Ulrich Scherhag, Luca Debiasi, Christian Rathgeb, Christoph\\nBusch, and Andreas Uhl. Detection of face morphing attacks\\nbased on PRNU analysis. IEEE Transactions on Biometrics,\\nBehavior, and Identity Science , 1(4):302–317, 2019.\\n[136] Quoc-Tin Phan, Giulia Boato, and Francesco GB De Natale.\\nAccurate and scalable image clustering based on sparse repre-\\nsentation of camera ﬁngerprint. IEEE Transactions on Infor-\\nmation Forensics and Security , 14(7):1902–1916, 2018.\\n[137] Haya R Hasan and Khaled Salah. Combating deepfake videos\\nusing blockchain and smart contracts. IEEE Access , 7:41596–\\n41606, 2019.\\n[138] IPFS powers the distributed web. https://ipfs.io/ .\\n[139] Akash Chintha, Bao Thai, Saniat Javid Sohrawardi, Kartavya\\nBhatt, Andrea Hickerson, Matthew Wright, and Raymond\\nPtucha. Recurrent convolutional structures for audio spoof and\\nvideo deepfake detection. IEEE Journal of Selected Topics in\\nSignal Processing , 14(5):1024–1037, 2020.\\n[140] Massimiliano Todisco, Xin Wang, Ville Vestman, Md Sahidul-\\nlah, H ´ector Delgado, Andreas Nautsch, Junichi Yamag-\\nishi, Nicholas Evans, Tomi Kinnunen, and Kong Aik Lee.\\nASVspoof 2019: Future horizons in spoofed and fake audio\\ndetection. arXiv preprint arXiv:1904.05441 , 2019.\\n[141] Shruti Agarwal, Hany Farid, Ohad Fried, and Maneesh\\nAgrawala. Detecting deep-fake videos from phoneme-viseme\\nmismatches. In Proceedings of the IEEE /CVF Conference on\\nComputer Vision and Pattern Recognition Workshops , pages\\n660–661, 2020.\\n[142] Ohad Fried, Ayush Tewari, Michael Zollh ¨ofer, Adam Finkel-\\nstein, Eli Shechtman, Dan B Goldman, Kyle Genova, Zeyu\\nJin, Christian Theobalt, and Maneesh Agrawala. Text-based\\nediting of talking-head video. ACM Transactions on Graphics\\n(TOG) , 38(4):1–14, 2019.\\n[143] Steven Fernandes, Sunny Raj, Rickard Ewetz, Jodh Singh\\nPannu, Sumit Kumar Jha, Eddy Ortiz, Iustina Vintila, and Mar-\\ngaret Salter. Detecting deepfake videos using attribution-based\\nconﬁdence metric. In Proceedings of the IEEE /CVF Confer-\\nence on Computer Vision and Pattern Recognition Workshops ,\\npages 308–309, 2020.\\n[144] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew\\nZisserman. VGGFace2: A dataset for recognising faces across\\npose and age. In The 13th IEEE International Conference on\\nAutomatic Face &Gesture Recognition (FG 2018) , pages 67–\\n74. IEEE, 2018.\\n[145] Susmit Jha, Sunny Raj, Steven Fernandes, Sumit K Jha,\\nSomesh Jha, Brian Jalaian, Gunjan Verma, and Ananthram\\nSwami. Attribution-based conﬁdence metric for deep neural\\nnetworks. Advances in Neural Information Processing Sys-\\ntems, 32:11826–11837, 2019.[146] Steven Fernandes, Sunny Raj, Eddy Ortiz, Iustina Vintila, Mar-\\ngaret Salter, Gordana Urosevic, and Sumit Jha. Predicting heart\\nrate variations of deepfake videos using neural ODE. In Pro-\\nceedings of the IEEE /CVF International Conference on Com-\\nputer Vision Workshops , pages 1721–1729, 2019.\\n[147] Pavel Korshunov and S ´ebastien Marcel. Deepfakes: a new\\nthreat to face recognition? assessment and detection. arXiv\\npreprint arXiv:1812.08685 , 2018.\\n[148] Shruti Agarwal, Hany Farid, Tarek El-Gaaly, and Ser-Nam\\nLim. Detecting deep-fake videos from appearance and behav-\\nior. In IEEE International Workshop on Information Forensics\\nand Security (WIFS) , pages 1–6. IEEE, 2020.\\n[149] Umur Aybars Ciftci, Ilke Demir, and Lijun Yin. FakeCatcher:\\nDetection of synthetic portrait videos using biological signals.\\nIEEE Transactions on Pattern Analysis and Machine Intelli-\\ngence , 2020.\\n[150] Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket\\nBera, and Dinesh Manocha. Emotions don’t lie: A deep-\\nfake detection method using audio-visual a ﬀective cues. arXiv\\npreprint arXiv:2003.06711 , 3, 2020.\\n[151] Luca Guarnera, Oliver Giudice, and Sebastiano Battiato. Deep-\\nfake detection by analyzing convolutional traces. In Proceed-\\nings of the IEEE /CVF Conference on Computer Vision and Pat-\\ntern Recognition Workshops , pages 666–667, 2020.\\n[152] Wonwoong Cho, Sungha Choi, David Keetae Park, Inkyu Shin,\\nand Jaegul Choo. Image-to-image translation via group-wise\\ndeep whitening-and-coloring transformation. In Proceedings\\nof the IEEE /CVF Conference on Computer Vision and Pattern\\nRecognition , pages 10639–10647, 2019.\\n[153] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,\\nSunghun Kim, and Jaegul Choo. StarGAN: Uniﬁed generative\\nadversarial networks for multi-domain image-to-image trans-\\nlation. In Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition , pages 8789–8797, 2018.\\n[154] Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan,\\nand Xilin Chen. AttGAN: Facial attribute editing by only\\nchanging what you want. IEEE Transactions on Image Pro-\\ncessing , 28(11):5464–5478, 2019.\\n[155] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\\nJaakko Lehtinen, and Timo Aila. Analyzing and improv-\\ning the image quality of stylegan. In Proceedings of the\\nIEEE /CVF Conference on Computer Vision and Pattern Recog-\\nnition , pages 8110–8119, 2020.\\n[156] Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik\\nLearned-Miller. Labeled faces in the wild: A database\\nfor studying face recognition in unconstrained environ-\\nments. Technical Report 07-49, University of Massachusetts,\\nAmherst, October 2007.\\n[157] Apurva Gandhi and Shomik Jain. Adversarial perturbations\\nfool deepfake detectors. In IEEE International Joint Confer-\\nence on Neural Networks (IJCNN) , pages 1–8. IEEE, 2020.\\n[158] Few-shot face translation GAN. https://github.com/\\nshaoanlu/fewshot-face-translation-GAN .\\n[159] Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen,\\nFang Wen, and Baining Guo. Face X-ray for more general\\nface forgery detection. In Proceedings of the IEEE /CVF Con-\\nference on Computer Vision and Pattern Recognition , pages\\n5001–5010, 2020.\\n[160] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew\\nOwens, and Alexei A Efros. CNN-generated images are\\nsurprisingly easy to spot... for now. In Proceedings of the\\nIEEE /CVF Conference on Computer Vision and Pattern Recog-\\nnition , pages 8695–8704, 2020.\\n[161] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and\\nLei Zhang. Second-order attention network for single image\\n18', metadata={'source': 'data/1909.11573.pdf', 'page': 17}),\n"," Document(page_content='super-resolution. In Proceedings of the IEEE /CVF Conference\\non Computer Vision and Pattern Recognition , pages 11065–\\n11074, 2019.\\n[162] Luca Guarnera, Oliver Giudice, and Sebastiano Battiato. Fight-\\ning deepfake by exposing the convolutional traces on images.\\nIEEE Access , 8:165085–165098, 2020.\\n[163] Todd K Moon. The expectation-maximization algorithm. IEEE\\nSignal Processing Magazine , 13(6):47–60, 1996.\\n[164] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.\\nUnpaired image-to-image translation using cycle-consistent\\nadversarial networks. In Proceedings of the IEEE international\\nconference on computer vision , pages 2223–2232, 2017.\\n[165] Ke Li, Tianhao Zhang, and Jitendra Malik. Diverse image syn-\\nthesis from semantic layouts via conditional imle. In Proceed-\\nings of the IEEE /CVF International Conference on Computer\\nVision , pages 4220–4229, 2019.\\n[166] Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva, Maria Li-\\nakata, and Rob Procter. Detection and resolution of rumours in\\nsocial media: A survey. ACM Computing Surveys (CSUR) , 51\\n(2):1–36, 2018.\\n[167] R. Chesney and D. K. Citron. Disinformation on steroids:\\nThe threat of deep fakes. https://www.cfr.org/report/\\ndeep-fake-disinformation-steroids , October 2018.\\n[168] Luciano Floridi. Artiﬁcial intelligence, deepfakes and a future\\nof ectypes. Philosophy &Technology , 31(3):317–321, 2018.\\n[169] Davide Cozzolino, Justus Thies, Andreas R ¨ossler, Christian\\nRiess, Matthias Nießner, and Luisa Verdoliva. ForensicTrans-\\nfer: Weakly-supervised domain adaptation for forgery detec-\\ntion. arXiv preprint arXiv:1812.02510 , 2018.\\n[170] Francesco Marra, Cristiano Saltori, Giulia Boato, and Luisa\\nVerdoliva. Incremental learning for the detection and classiﬁ-\\ncation of gan-generated images. In 2019 IEEE International\\nWorkshop on Information Forensics and Security (WIFS) ,\\npages 1–6. IEEE, 2019.\\n[171] Shehzeen Hussain, Paarth Neekhara, Malhar Jere, Farinaz\\nKoushanfar, and Julian McAuley. Adversarial deepfakes: Eval-\\nuating vulnerability of deepfake detectors to adversarial exam-\\nples. In Proceedings of the IEEE /CVF Winter Conference on\\nApplications of Computer Vision , pages 3348–3357, 2021.\\n[172] Nicholas Carlini and Hany Farid. Evading deepfake-image de-\\ntectors with white-and black-box attacks. In Proceedings of the\\nIEEE /CVF Conference on Computer Vision and Pattern Recog-\\nnition Workshops , pages 658–659, 2020.\\n[173] Chaofei Yang, Leah Ding, Yiran Chen, and Hai Li. Defending\\nagainst gan-based deepfake attacks via transformation-aware\\nadversarial faces. In IEEE International Joint Conference on\\nNeural Networks (IJCNN) , pages 1–8. IEEE, 2021.\\n[174] Chin-Yuan Yeh, Hsi-Wen Chen, Shang-Lun Tsai, and Sheng-\\nDe Wang. Disrupting image-translation-based deepfake al-\\ngorithms with adversarial attacks. In Proceedings of the\\nIEEE /CVF Winter Conference on Applications of Computer Vi-\\nsion Workshops , pages 53–62, 2020.\\n[175] M. Read. Can you spot a deepfake? does it matter? http:\\n//nymag.com/intelligencer/2019/06/how-do-you-\\nspot-a-deepfake-it-might-not-matter.html , June\\n2019.\\n[176] Marie-Helen Maras and Alex Alexandrou. Determining au-\\nthenticity of video evidence in the age of artiﬁcial intelligence\\nand in the wake of deepfake videos. The International Journal\\nof Evidence &Proof , 23(3):255–262, 2019.\\n[177] Lichao Su, Cuihua Li, Yuecong Lai, and Jianmei Yang. A fast\\nforgery detection algorithm based on exponential-Fourier mo-\\nments for video region duplication. IEEE Transactions on Mul-\\ntimedia , 20(4):825–840, 2017.\\n[178] Massimo Iuliani, Dasara Shullani, Marco Fontani, SaverioMeucci, and Alessandro Piva. A video forensic framework\\nfor the unsupervised analysis of MP4-like ﬁle container. IEEE\\nTransactions on Information Forensics and Security , 14(3):\\n635–645, 2018.\\n[179] Badhrinarayan Malolan, Ankit Parekh, and Faruk Kazi. Ex-\\nplainable deep-fake detection using visual interpretability\\nmethods. In The 3rd International Conference on Information\\nand Computer Technologies (ICICT) , pages 289–293. IEEE,\\n2020.\\n[180] Oliver Giudice, Luca Guarnera, and Sebastiano Battiato. Fight-\\ning deepfakes by detecting GAN DCT anomalies. arXiv\\npreprint arXiv:2101.09781 , 2021.\\n19', metadata={'source': 'data/1909.11573.pdf', 'page': 18}),\n"," Document(page_content='DUJASE Vol. 5(1 & 2) 1 3-23, 2020 (January & July)  \\nDeep Insights of Deepfake Technology : A Review  \\nBahar Uddin Mahmud1* and Afsana Sharmin2 \\n1Department of CSE , Feni University , Feni, Bangladesh  \\n2Department of CSE , Chittagong Universit y of Engineering & Technology ,Bangladesh  \\n*E-mail: mahmudbaharuddin@gmail.com  \\nReceived on 18 May 2020 , Accepted for publication on 10 September 2020  \\nABSTRACT  \\nUnder the aegis of  computer vision and deep learning technology, a new emerging techniques has introduced that \\nanyone can make highly realistic but fake videos, images even  can manipulates the voices. This technology is widely \\nknown as Deepfake Technology. Although it seems interesting techniques to make fake videos or image of something \\nor some ind ividuals but it could spread as misinformation via internet. Deepfake contents could be dangerous for \\nindividuals as well as for our communities, organizations, countries religions etc. As Deepfake content creation \\ninvolve a high level expertise with combi nation of several algorithms of deep  learning,  it seems  almost  real and \\ngenuine and difficult to differentiate. In this paper, a wide range of articles have been examined to understand \\nDeepfake technology more extensively. We have examined several articles  to find some insights such as what is \\nDeepfake, who are responsible for this, is there any benefits of Deepfake and what are the challenges of this \\ntechnology. We have also examined several creation and detection techniques. Our study  revealed  that althou gh \\nDeepfake is  a threat to our societies, proper measures  and strict regulations could prevent this.   \\nKeywords:  Deep learning, Deepfake, review, Deepfake  generation, Deepfake creation  \\n \\n1. Introduction  \\nDEEPFAKE, combination of deep learning and fake con - \\ntents, is a process  that involve s swapping of a face from a \\nperson to a targeted person in a video and make the face ex - \\npressing similar to targeted person and act like targeted \\nperson saying those words that actually said by another \\nperson. Face swapping specially on image and video or \\nmanipulation of facial expression  is called as Deepfake \\nmethods [1]. Fake videos and images that go over the \\ninternet can easily exploit some individuals and it becomes \\na public issue recently [24]. Creating false contents by \\naltering the face  of an indivi dual referring as source in an \\nimage or a video of another person referred as target is \\nsomething that called DeepFake, which was named after a \\nsocial media site Reddit account name “Deepfake” who \\nthen claimed to develop a machine learning technique to \\ntransfer celebrity faces into adult contents [5]. Furthermore, \\nthis technique is also used to create fake news, fraud and \\neven spread hoaxes. Recently this field has  got special \\nconcern of researcher who are now dedicatedly involve in \\nbreaking out the insight s of Deepfake [6 -8]. Deepfake \\ncontents can be pornography, political or bullying of a \\nperson by using his or her image and voice without his or \\nher consent [9].There are several models in deep learning \\ntechnology for instance, autoencoders and GAN to deal \\nwith several problems of computer vision  domain [10 -\\n13].Several Deepfake algorithms have been proposed using \\ngenerative adversarial networks to copy movements and \\nfacial expressions of a person and swap it with another \\nperson [14]. Political person, public  figure, celebrity are the \\nmain victims of Deepfake. To spread false message of \\nworld leaders Deepfake technology used several times and \\nit could be threat to world peace [15]. It can be used to \\nmislead military personnel by providing fake image of \\nmaps an d that could create serious damage to anyone [16]. Howerver, there are many promising aspects of this \\ntechnology. People who have lost their voice once can get it \\nback with the help of this technology [17]. As we know  \\nfalse news spreads faster and wider bu t it is difficult task  to \\nrecover it [18]. To  know  Deepfakes  properly  we have to \\nknow about it deeply such as what Deepfake actually is, \\nhow it comes, how to create it and how to detect it etc. As \\nthis field is almost new to researcher just introduced in \\n2017 there are not enough resources on this topics. \\nAlthough several research have introduced recently to deal \\nwith social media misinformation related to Deepfake [19]. \\nIn this paper after the introduction ,we discussed more \\nabout D eepfake technology and i ts uses . And then we \\ndiscussed possible threats and challenges of this \\ntechnology, thereafter, we discussed different articles \\nrelated to Deepfake generation and Deepfake detection and \\nalso its positive and negative sides . Finally, we discussed \\nthe limitat ions of our work, suggestions and future \\nthoughts.  \\nA. Deepfake  \\nDeepfake, a mixtures of deep learning and fake, are \\nimitating contents where targeted subject‟s face was \\nswapped by source person to make videos or images of \\ntarget person [20 -21]. Although, makin g of fake content is \\nknown to all  but Deepfake is something beyond someone‟s \\nthoughts which make this techniques more powerful  and \\nalmost  real using ML and AI to manipulate original content \\nto make fraud one [22 -24]. Deepfake has huge range of \\nusages such as making fake pornography of well known \\ncelebrity, spreading of fake news, fake voices of politicians, \\nfinancial fraud even many more [25 -27]. Although face \\nswapping technique is well known in  the film industry \\nwhere several fake voice or videos  were  made  as their \\nrequirement but that took huge time and certain level of \\nexpertise. But through deep learning techniques, anybody ', metadata={'source': 'data/Deep_Insights_of_Deepfake_Technology_A_Review.pdf', 'page': 0}),\n"," Document(page_content='14  Bahar Uddin Mahmud and Afsana Sharmin  \\nhaving sound computer knowledge and high configuration \\nGPU computer can make trustworthy fake videos or \\nimages.  \\nB. Application of  Deepfake  \\nDeepfake technology has a huge range of applications \\nwhich could use both  positively or negatively,  however  \\nmost of the time it is used for malicious purposes. The \\nunethical uses of Deepfake technology has harmful \\nconsequences in our society eithe r in short term or long \\nterm. People regularly using social media are in a huge r isk \\nof Deepfake. However , proper use of this technology could \\nbring many positive results. Below both negative and \\npositive applications of Deepfake technology  are described \\nin details.  \\n1) Negative Application :  Deepfake and technology \\nrelated  to this are expanding rapidly in current years. It \\nhas ample  applications that use for malicious work against \\nany human being especially against celebrity and  political \\nleaders. There are several reasons behind making \\nDeepfake content that could be out of fun but sometimes it \\nis used for taking revenge, blackmailing, stealing identity \\nof someone and many more. There are thousands videos \\nof Deepfake and most of them are adult videos of women \\nwithout their permission [28]. Most common use of \\nDeepfake technology is  to make pornography of well \\nknown actress and it is rapidly increasing day by day \\nspecially of Hollywood actresses [29]. Moreover, in 2018 \\na software wa s built that make a women nude in a single \\nclick and it widely went viral for malicious purposes to \\nharass women [30]. The another most malicious use of \\nDeepfake is to exploit world leaders and politician by \\nmaking fake videos of them and sometimes it coul d have \\nbeen great risk for world peace. Almost all world leader \\nincluding Barack Obama, former president of USA, \\nDonald trump, running president of USA, Nency Pelosi,  \\nUSA based politician, Angela Merkel, German chancellor \\nall are exploited by fake videos s omehow and even \\nFacebook founder Mark Zukerberg have faced similar \\noccurrence  [31]. There are also vast use of Deepfake in \\nArt, film industry and in social  media.  \\n2) Positive Application: Although most of the time this  \\ntechnology is used for malicious work wi th bad intention \\nstill it has some positive uses also in several sector s. The \\nDeepfake creation is no longer remain limited to experts, \\nit is now become much more easier and accessible to \\nanyone. Nowadays constructive uses of this technology \\nwidely increas ed. To create new art work, engage \\naudiences and give them unique experiences this \\ntechnology  was use [32]. Recently the Dal´ı Museum in \\nSt. Petersburg, Florida created a chance to its visitor to \\nmeet Salvador Dal´ı and engage with his life more \\ninteractiv ely to know this great personality via artificial \\nintelligence[33]. Deepfake technology now use d both in \\nadvertising and business purposes too. Technologists now \\nare using the Deepfake to make copy of famous artwork \\nsuch as creating video of famous Monalis a artwork using \\nthe image [34]. Deepfake technology can save huge money and time of film industry by using the capabilities \\nof Deepfake technology for editing videos rather than re -\\nshot and there are many more positive examples such as \\nfamous footballer Da vid Becham spoke in 9 different \\nlanguages to run a campaign against malaria and it has \\nalso positive aspects in education sector [35].  \\nGANS can be used in various field to give realistic \\nexperiences such as in retail sector, it might be possible to \\nsee the  real product what we see in shop going physically \\n[36]. Recently Reuters collaborated with AI startup \\nSynthesis has made first ever synthesized news presenter by \\nusing artificial intelligence techniques and it was done \\nusing same techniques that  is used i n Deepfakes and it \\nwould be helpful for personalized news for individuals [37]. \\nDeep generative models also has shown great possibilities \\nof de velopment in health care indus try. To protect real data \\nof patients and research work instead of sharing real dat a \\nimaginary data could be generated via  this technology [38]. \\nAdditionally, this technology has great potential in \\nfundraising and awareness building by making videos of \\nfamous personality who is asking for help or fund for some \\nnovel work [39].  \\n2. Deepfake Generation  \\nDeepfake techniques involve several deep learning \\nalgorithm to generate fake contents in the form of videos, \\nimages, texts or voices. GAN are used to produce forged \\nimages and videos. Nowadays Deepfake technology \\nbecome more popular due to its e asiness and cheap \\navailability. And also wide range of applications of \\nDeepfake techniques create special interest among \\nprofessional as well as novice users. A widely used model \\nin deep network is deep autoencoders that has 2 uniform \\ndeep belief networks where 4 or 5 layers represent the \\nencoding half and rest represent the decoding half. Deep \\nencoding widely used in reduction of dimensions and \\ncompression of images [40] [41].First ever success of \\nDeepfake technique was creation of FakeApp which is used \\nto swap a person face with another person [42]. “FakeApp” \\nsoftware needs huge chunks of data for  better  results  and \\ndata is given to the system to train the  model  and then \\nsource person face inserts  into the targeted  video. Creation  \\nof fake videos in Fake  App involves extraction of all \\nimages from source video in a  folder,  properly  crop them  \\nand make alignment and then processed it with trained  \\nmodel, after merging the faces, the final video become \\nready [43]. Garrido et al. [44] proposed a method that use \\ncustomized join structure method of the actor in the video \\nas a shape representation, the overlay of blend  shape  model  \\nis refined in a temporally coherent way and after that high \\nfrequency shape detail reconstructed using photometric \\nstereo approach that exc eeds unknown light. Comparing \\nwith the technique discussed by Valgaerts et al. [45] that \\nused binocular facial  subject this method [44] show  high \\nshape  details. Chen Cao  et al. [46] proposed a process for  \\npractical facial finding  and animation where video stream \\nof source  sent through user specific 3D shape regresser for \\n3D facial shape and for simultaneously compute face shape ', metadata={'source': 'data/Deep_Insights_of_Deepfake_Technology_A_Review.pdf', 'page': 1}),\n"," Document(page_content='Deep Insights of Deepfake Technology : A Review  15 \\nand motions, user blend shapes and camera matrix using the \\ntechnique displaced dynamic expression(DDE) Chen Cao et \\nal. [47] propose d a real time high  fidelity facial capture \\nsystem that able to reconstruct person specific ankle details \\nin real time from a single camera. It automatically \\ninitializes person specific wrinkle probability map in any \\nuncontrolled set of expressions.  It is b ased on global face \\ntracker which generates low resolution mesh. Then the \\nmesh tracking is improved and enhanced the details through \\nnovel local wrinkle regression methods. It can generates \\nrealistic facial wrinkles and also a globally plausible face \\nshape  which can be seen from any dimension. Comparing \\nto Chen Cao  et al. [46] it has low tracking error, with this \\nerror there are temporal noise and artifacts on final results. \\nComparing to high resolution capture method of Beeler et \\nal. [48] this experiment d id not  capture every sort of details \\nbut this real time results is a plausible review without any \\ndepth constraints. Comparing to offline monocular capture \\nmethod proposed by Garrido et al. [44] this experiment‟s \\nresults is clearly better in quality. Justu s Thies et al. [49] \\nintroduced a method uses a new model based tracking \\napproach based on a parametric face model. This model \\ntracked from real time RGB -D input. In this experiment \\nface model calibration for each actor needs  before the \\ntracking start. Sinc e mouth changes shape in the target this \\nexperiment showed synthesizes new mouth interior using \\nteeth proxy and texture of the mouth interior. Transfer  \\nquality is high even if the lighting quality in source and \\ntarget differs in this method. Comparing with  “Face  Shift” \\nof Weise et al. [50], although the geometric alignment  of \\nboth approaches is similar, this method achieve \\nsignificantly better photo metric alignment. Comparing \\nwith Chen Cao et al. [46] (RGB only), this model‟s RGB -D \\napproach reconstruct mod el can shape expression details of \\nthe actor more closely.  \\nFace  2 face, a real time facial reenactment method proposed \\nby Justus Thies et al. [51] that works for any commodity \\nwebcam. Since this method uses only RGB data for source  \\nand target actor, it can  manipulate real time Youtube  video.  \\nA significant difference to previous method Justus Thies et  \\nal. [49] is the re rendering the mouth interior, for this, the \\nauthors designed internal side of lip of the target using \\nvideo footage from the training sequen ce based on temporal \\nand photo metric similarity. The system reconstruct and \\ntracks both source and target actor using dense photo -metric \\nenergy minimization. Using a novel subspace deformation \\ntransfer technique it shifts  appearance from origin to \\ndestina tion, then it re rendered the  modified  face on the top \\nof target sequence in order to replace the original facial \\nexpressions. This method introduced new RGB pipeline \\nwhich pre compared against modern real time face tracking \\nprocess. Comparing to Thies el al. [49] and Cao et al. [46], \\nit is noted that first one used RGB -D and second one and  \\nthis method only use RGB data. In comparison with offline  \\ntracking algorithm of Shi et al. [52], it is found that that \\nmethod used additional geometric refinement using shading \\ncures. Comparing this method with Garrideo et al. [53] \\noffline works, it is found that both method results similar reenactment data however this uses a geometric teeth proxy \\nwhich leads artificially shift mouth region. Supasorn \\nSuwajanakorn et al. [54] used the recurrent neural network \\nthat transform input video to time varying mouth shape. \\nAfter synthesizing mouth texture the chips was enhanced in \\ndetails.  \\nFinally they blend the mouth texture on to a re time target \\nvideo and match the pose. As thi s approach requires only \\naudios it can generate high quality head close up even the \\noriginal video has low resolution. This approach works on \\ncasual speech also. Comparing with the face2face [51], this \\nmethod works better in some case such as face2face nee d \\ninput video to drive the animation of target person where \\nthis approach needs only audio speech. Justus Thies et al. \\n[55] proposed a method that Enables full control over \\nportrait video. It is based on short  RGB -D video sequence \\nof the destination person . From this sequence proxy mesh is \\nextracted using voxel hashing. To generate an automated \\nrig face template automatically fix to this mesh. Using \\ndense correspondences between face mesh and scan mesh \\nblend shaped has been transferred to proxy. And the \\nautomated rig enables fully animate the target proxy mesh.  \\nView dependent image synthesis proposed for deformed \\ntarget actor .Then the target proxy mesh partitioned into \\nthree parts head, neck and body. For this three part the \\nnearest texture retrieves from t he initial video sequence. \\nThen run the mesh into the space of target image \\ncompensating for the incomplete reconstruction by using \\nimage wrapped field. Then all three parts are composited to \\ngenerate desire output image. Source actor expression and \\nmoveme nt are tracked by using multi linear model for face \\nand rigid proxy model for upper body and then the \\nparameters are transferred to target actor‟s rig and rendered \\nit. The eye texture are retrieved from the calibration \\nsequence of target actor using the ey e motion of source \\nactor. Tab. 1 describes several Deepfake generation related \\narticle.  \\nA. Several Tools for Deepfake Generation  \\nThere are several tools and software s available for \\nDeepfake video generation. Face  Swap [56] is one of the \\nopen source software that use deep learning mechanism to \\nswap faces in images or videos. Concept of Generative \\nadversarial networks (GANS) is used which includes two \\nencoder and decoder pairs. In this techniques parameters of \\nencoders are shared. An improved  version  of previous  tool \\nis FaceSwap -GAN  [57],  that VGGface  to Deepfakes auto -\\nencoding architecture. It supports several output resolutions \\nsuch as 64x64, 128x128, and 256x256. It  is capable of \\nrealistic and consistent eye movements. It used VGGFace \\nperceptual loss [58] concepts that helps to improve \\ndirection of eyeballs to be more precise and in line with \\ninput face. This tool includes MTCNN face detection \\ntechnique[59] and kalman  filter [59] to detect more stable \\nface and smoothen it respectively. “Faceswap -pytorch” [60] \\nanother Deep - fake creation tool that makes dataset loader \\nmore efficient can load from 2 directory simultaneously. \\nNew face outline replace technique is used to get a much ', metadata={'source': 'data/Deep_Insights_of_Deepfake_Technology_A_Review.pdf', 'page': 2}),\n"," Document(page_content='16  Bahar Uddin Mahmud and Afsana Sharmin  \\nmore combination result with original image. Most used \\ntool for making Deepfake videos is “DeepFaceLab” [61], \\nthat runs much faster than previous version. It has more \\ninteractive converter. “DFaker” [62] is another tool that \\nused DSSIM loss func tion [63] to reconstruct face.  Most of \\nthe face swapping tools use the concepts of Generative \\nadversarial networks(GANS). From [64, fig. 1 ], it is the \\nvisual diagram of encoder in GANS. It has the ability to \\nextract deep information from image, in encodin g stage it \\nextracts the facial expressions of two individual. Then it \\nanalyze the common features between two faces and \\nmemorizes it. After that, in decoding stage in [64,fig.2], it \\ndecodes the information of two individuals. And in [64,fig. \\n3] shows the w orkings of discriminator that used to make \\ndecision of authenticity of features of every given data. \\nCopy the source features to target features is most tedious \\ntask in face swapping and it is done by autoencoder after \\nproper training. A implicit layer ins ide the encoder trained to generate the code to represent input. And auto encoder \\ncomprise of 2 slice encoder that represent input and decoder \\nthat generates the reconstruction. As represent in [64,fig. 4] \\nand face swapping tools described here mostly used  \\nautoencoder technique. The main objectives of autoencoder \\nis to handle MAE loss(reconstruction loss), adversarial loss( \\nperforms by Discriminator) and perceptual loss  (which \\noptimize similarity between source image and target \\nimage).  \\n3. Deepfake Detection  \\nIt is often difficult  and sometimes impossible to detect \\nDeepfake contents by a human being with untrained eye s. A \\ngood level of expertise is needed to detect irregularities in \\nDeep fake videos. Till now several approaches have been \\nproposed including machin e detection, forensics, \\nauthentication as well as regulation to combat Deepfake.  \\n \\nTable 1: Summary of remarkable deepfake related articles  \\n \\n', metadata={'source': 'data/Deep_Insights_of_Deepfake_Technology_A_Review.pdf', 'page': 3}),\n"," Document(page_content='Deep Insights of Deepfake Technology : A Review  17 \\n \\nFig. 1.  The encoding technique of face swap tools   \\n \\nFig. 2.  The decoding technique of face swap tools.  \\nExperts say as Deepfake video is created by algorithm, \\nwhile real video is made by a actual camera, it is possible to \\ndetect Deepfake from existing clues and artifacts. There are \\nalso some anomalies like lighting inco nsistencies, image \\nwarping, smoothness in areas and unusual pixel formations \\nwhich could help to detect Deepfake. Detecting Deepfakes \\nKorshunov et al. [65] described a process that use to find \\ninconsistency in the middle of visible mouth motion and \\nvoice i n recording. In this article they also applied several \\napproaches including simple principal component analysis \\n(PCA) , linear discriminant analysis (LDA), image quality \\nmetrics (IQM) and support vector machine (SVM) [66]. \\nVidTIMIT database [67], is a publ icly available database of \\nvideos, used for generating several Deepfakes videos with \\ncombination of different features of Deepfake creation \\ntechnique including face swapping, mouth movements, eye \\nblinking etc. After using these videos to verify several methods of Deepfake detection, it is found that several face \\nswap identification technique fail to detect fake contents.  \\n \\nFig. 3. Examin ation  process of instances of given data  \\n \\nFig. 4. Working procedures of autoencoder.  \\nAs example deep learning based face recognition technique \\nVGG [68] and Facenet [69] are unable to detect Deepfakes \\nproperly. Although earlier Deepfake detection method was \\nnot able to measure the blink of eye, recent methods \\nshowed promising results to detect  eye blink in source \\nvideo and ta rget video. The authors in [70] aimed to detect \\nfacial forgeries automatically and properly by using recent \\ndeep learning techniques convolutional neural networks \\n(CNNs) with the help of neural network.  \\nForensics model face s extreme difficulties to detect \\nforgeries when the source data are made by CNN  and GAN \\ndeep learning method [71]. To avoid the problems of \\nadaptability, Huy H. Nguyen et al.  proposed  a method that \\nsupports generalization and could locate manipulated \\n', metadata={'source': 'data/Deep_Insights_of_Deepfake_Technology_A_Review.pdf', 'page': 4}),\n"," Document(page_content='18  Bahar Uddin Mahmud and Afsana Sharmin  \\nlocation easily. Ekraam Sabir et al. [72] tried to detect face \\nswapping that generate by several available software such \\nas Deepfake, face2face, faceswap by using conventional \\nnetworks and recurrent unit. Deepfake contents could be in \\nimage format or video format. There are lots of research t o \\ndetect fake videos and images. [73 -76, Tab. 2] describes the \\nseveral remarkable Deepfake articles that worked on \\ndetection.  \\nA. Detection based on several artifacts  \\nThere are lots of research s that have done and thousands are \\nongoing to combat fake contents detection. Several artifacts \\nsuch as head movements, facial expression, eye blinking are \\nthe most demanding subjects to the researcher in detecting \\nfake videos. Yuezun Li et al. [73] have pr oposed a method  \\nthat detect Deepfake using convolutional neural networks . \\nIn this paper they discussed the method to detect the \\ndistinction between fake video and real video based on \\nresolution. In comparison with Headpose [74], this method \\nshows better r esults as only detecting head pose does not \\nwork for frontal face. Li et al. [75] described a process that \\ntraced out eye blinking in fake video and detect the \\nDeepfake videos based on CNN/RNN model. Shruti \\nAgarwal et al. [76] discussed  a forensic techniqu e to detect \\nDeepfakes by capturing facial expression and head \\nmovements. It showed better results from some other \\ndetection technique in some cases. In [73, fig. 5], it shows \\nthe detection of artifacts between original faces and \\ngenerated faces by making t he comparison between the face \\nsurroundings and other face regions with a dedicated CNN \\nmodel.  B. Binary Classifier based fake video detection  \\nHuy H. Nguyen et al. [77] proposed capsule network based \\nsystem to detect forged videos and images using the \\nconcept s of Capsule networks. Capsule networks able to \\ngenerate the spatial relationships between several face parts \\nand a complete face and by using concepts of deep \\nconvolutional neural networks, it able to detect various \\nkinds of spoofs. In this paper the auth ors discussed the \\nReplay Attack Detection,  Face Swapping Detection, Facial \\nReenactment Detection pro cesses. Andreas  Ro¨ssler  et al. \\n[78] discussed  a detection  tool that helps to find hoax \\ncontents generate by several deepfake generation tools such \\nas Deep fakes, Face2Face, FaceSwap and Neural Texture. \\nThis method is able to check the region of face landmark \\nperfectly. David Guera et al. [79] discussed the method of \\nfake detection by using recurrent neural network where \\nwhere they used CNN for features extra ction on frame level \\nand RNN to grab anomalies of frames. Mengnan Du et al. \\n[80] proposed e Locality -aware  Auto Encoder (LAE), that \\nmerge close depiction studying and implementing setting in \\na united framework. It makes predictions relying on correct \\nevide nce in order to boost generalization accuracy. Davide \\nCozzolino et al.  [81] discussed a forgery detection \\ntechnique that introduced  a method based on neural \\nnetwork which is used to transfer between several \\nmanipulation domain. It also described how transf erability \\nenables robust  forgery  detection.  In [ 78, fig.6 ], it shows \\nthe whole working process of previously discussed \\ndetection tool FaceForensic++ which used CNN model for \\nreenactment, replacement and manipulation of fake \\ncontents.  \\n \\nTable 2:  Summary of several deepfake detection related articles  \\n \\n', metadata={'source': 'data/Deep_Insights_of_Deepfake_Technology_A_Review.pdf', 'page': 5}),\n"," Document(page_content='Deep Insights of Deepfake Technology : A Review  19 \\nTemporal discrepancies in the frames are discovered using \\nR-CNN which combines convolutional network DenseNet \\nand recurrent unit cells [83]. LRCN based eye blinking \\n[75], that learn temporal patterns of eye blinking, found \\nfrequency of blinking in Deepfake video is less than \\noriginal. To  discover resolution inconsistency between real \\nface and  its surrounding, face warping artifacts [73] is used  \\nthat built  on using techniques VGG 16 [84], ResNet50, 101  \\nor 152 [85]. Capsule forensics [77] is introduced to extract \\nfeatures from face by VGG -19 network are classified by \\nfeeding into capsule networks. Through a numbers of \\nlooping, output of the three convolutional capsules is routed \\nwith the help of a dynamic routing algorithm [86] and as a \\nresult two output capsules has been generated, one for fake \\nand another for real image. Combination of CNN and \\nLSTM is applied in intra -frame and temporal \\ninconsist encies [79] where CNN extracts fame level \\nfeatures and LSTM construct sequence descriptor. Pairwise \\nlearning [87] used CNN concatenated to CFFN Two -phase \\nprocedure one is feature extraction using Siamese network \\narchitecture [88] another is classification using CNN. \\nMesoNet [89], which used the CNN technique ,introduced 2 \\ndeep networks Meso -4 and MesoInception -4 to detect fake \\nvideo content. Generalization of GAN technique [90] used \\nDCGAN,WGANGP and PGGAN enhances this \\ngeneralization ability of model. It is  used to remove low \\nlevel features of images and focus on pixel to pixel \\ncomparison in the middle of hoax  and actual  images. Zhang  \\net al. [91] introduced a classifiers that used SVM,  RF, MLP \\nto extract discriminant features and classify real vs \\nfabricated.  Eye, teach and facial texture [92] used logistic \\nregression and neural network for classify several facial \\ntexture and compare with real one. PRNU Analysis [93] \\nproposed to detect PRNU patterns between fake and \\nauthentic videos.  \\n \\nFig. 5.  Detecting Face Warping Artifacts.  \\n \\nFig. 6.  Detection of manipulated image.  \\n4. Discussions and Conclusions  \\nIn this paper , we have discussed several existing Deepfake \\nvideos and images generation and detection technique and \\napplication of Deepfakes  for both positive and negative  perposes . We also discussed several tools available for \\nDeepfake creation and detection. Technology is making \\ntremendous improvement day by day and every day new \\ndimension of technology is coming to us.  Improvement of \\nDeepfake generat ion technique make the detection work \\ndifficult day by day. Improved detection tech - niques and \\naccurate dataset are important issues for detecting Deepfake \\nproperly [94] [95]. From our study it is shown that Because \\nof Deepfake technology, people are loos ing trust on online \\ncontent day by day. As Deepfake contents creation \\ntechnique improving day by day, any person having a high \\nconfiguration computer instead of having less technological \\nknowledge could create Deepfakes content of any \\nindividuals for malic ious purposes. Moreover, the \\nadvancement of internet and networking made it possible to \\nspread Deepfake videos  in a moment. This technique could \\ninfluence the decisions  of world leaders, important public \\nfigures which could be harmful for world peace [96].  From \\nour study, we found that the battle between Deepfake \\ncreator and detector is growing rapidly. Although having \\nlots of negativity, we have also dis - cussed several positive \\nuse of Deepfake technology such as  in film industry, \\nfundraising etc. It is po ssible to return back  the voice of a \\nindividual who have already lost it by using  the Deepfake \\ngeneration methods. There are lots of debate against and in \\nfavor of Deepfake technology. Our study tried to analyze \\nthe both sides of Deepfake technology. DeePf ake \\ntechnology have a detrimental effects on film industry, \\ncommercial media platform, social media. Deepfake \\ntechnology used to gain trust of people via social media in \\nany political context or any other social media context. To \\nmake profit by generating traffic of fake news via web \\nplatform is increasing rapidly [97]. Zannettou et al. [98] \\nshowed a number of people behind Deepfake including \\npoliticians, public figures, celebrity ,creating Deepfakes and \\nspreading it via social media for various beneficial purposes \\nof their own. We have also found that many organizations \\nare involved in making Deepfake contents for useful \\npurposes. According to our study Deepfake has lots of \\nthreats towards individuals, society, politics as well as \\nbusiness. Because of incre asing fake contents , the situation \\nbecome worse for media person to detect real one specially \\njournalists.  \\nOn the other hand, we have found that proper development \\nof anti development technology, proper rules regulation and \\nawareness could combat Deepfake  for malicious uses. \\nQuick responses to fake contents uploaded in any social \\nmedia platform is important to prevent further spreading. \\nBy building awareness among people and educate people \\nabout the literacy of Deepfake could lessen the expansion \\nof malici ous uses of this technology. Governments and \\nseveral companies should run campaigning to build \\nawareness against misuse of this technology. Furthermore, \\ntechnological tools for detection and prevention Deepfake \\nmust be improved.  \\nAt the end, there are obvio usly some limitations of our \\nfindings. We have discussed certain articles, tools related \\nto Deepfake technology. Through our paper we have \\n', metadata={'source': 'data/Deep_Insights_of_Deepfake_Technology_A_Review.pdf', 'page': 6}),\n"," Document(page_content=\"20  Bahar Uddin Mahmud and Afsana Sharmin  \\nreviewed quite good numbers of articles to understand the \\nDeepfake generation techniques and current situation of \\nthis technology, its benefits and harms but still there are \\nlot of scholarly papers, articles of this technology  to cover  \\nto find more depth knowledge of this technology. Also we \\nhave just studied the articles briefly and tried to present \\nthe important aspect s but extensive study would have \\npresent more better analysis. There are lots of analysis \\navailable of this technology and deep insights to this will \\nprovide better opportunities for further research in this  \\nfield.  \\nReferences  \\n1. P. Korshunov and S. Marcel, “Deepfakes: a New Threat to \\nFace Recogni - tion? Assessment and Detection,” arXiv \\npreprint arXiv:1812.08685,2018  \\n2. J. A. Marwan Albahar, Deepfakes Threats and \\nCountermeasures Sys - tematic Review, JTAIT, vol. 97, no. \\n22, pp. 3242 -3250, 30 November 2019.  \\n3. Cellan -Jones, R., 2020. Deepfake Videos 'Double In Nine \\nMonths'. [online] BBC News. Available at: \\n<https://www.bbc.com/news/ technology -49961089> \\n[Accessed 5 March 2020].  \\n4. Danielle Citron. 2020.  TED Talk | Danielle Citron . [online] \\nAvailable at: <https://www.dani ellecitron.com/ted -talk/> \\n[Accessed 5 march 2020].  \\n5. BBC Bitesize. 2020. Deepfakes: What Are They And Why \\nWould I Make One?. [online] Available at: \\n<https://www.bbc.co.uk/bitesize /articles/zfkwcqt> \\n[Accessed 7 March 2020].  \\n6. Stamm, M. and Liu, K., 2010. Foren sic detection of image \\nmanipulation using statistical intrinsic fingerprints. IEEE \\nTransactions on Information Forensics and Security, 5(3), \\npp.492 -506. \\n7. J. Stehouwer, H. Dang, F.  Liu, X. Liu, and A. Jain, “On the \\nDetection  of Digital Face Manipulation,” ar Xiv preprint \\narXiv:1910.01717, 2019.  \\n8. A. Rossler,  D. Cozzolino,  L. Verdoliva,  C. Riess,  J. Thies,  \\nand ¨M. Nießner, “FaceForensics++: Learning to Detect \\nManipulated Facial Images,” in Proc. International \\nConference on Computer Vision, 2019.  \\n9. Fletcher,  J. 2018.  Deepfakes,  Artificial  Intelligence,  and \\nSome  Kind of  Dystopia:  The New  Faces  of Online  Post-Fact \\nPerformance. Theatre Journal, 70(4): 455 –471. Project \\nMUSE, https://doi.org/10.1353/tj.2018.0097  \\n10. Guo, Y., Jiao, L., Wang, S., Wang, S. and Liu, F., 2018. \\nFuzzy Sparse Autoencoder Framework for Single Image Per \\nPerson Face Recognition. IEEE Transactions on Cybernetics, \\n48(8), pp.2402 -2415.  \\n11. Yang, W., Hui, C., Chen, Z., Xue, J. and Liao, Q., 2019. FV -\\nGAN: Finger Vein Representation Using Generative \\nAdversarial Netwo rks. IEEE Transactions on Information \\nForensics and Security, 14(9), pp.2512 -2524.  \\n12. Cao, J., Hu, Y., Yu, B., He, R. and Sun, Z., 2019. 3D Aided \\nDuet GANs for Multi -View Face Image Synthesis. IEEE \\nTransactions on Information Forensics and Security, 14(8), \\npp.2028 -2042.  13. Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X. \\nand Metaxas, D., 2019. StackGAN++: Realistic Image \\nSynthesis with Stacked Generative Adversarial Networks. \\nIEEE Transactions on Pattern Analysis and Machine \\nIntelligence, 41(8), pp.1947 -1962.  \\n14. Lyu, Lyu, S., 2020. Detecting 'Deepfake' Videos In The Blink \\nOf An Eye. [online] The Conversation.Available at: \\n<https://theconversation.com/ detecting -deepfake -videos -in-\\nthe-blink -of-an-eye-101072> [Accessed 7 March 2020].  \\n15. Chesney, R. and Citron, D. , 2020. Deepfakes And The New \\nDisinformation War. [online] Foreign Affairs. Available at: \\n<https://www.foreignaffairs.com/articles/world/2018 -12-\\n11/deepfakes -and-new-disinformation -war> [Accessed 7 \\nMarch 2020].  \\n16. Fish,  T. (2019,  April  4). ”Deep  fakes:  AI-manipulated  media  \\nwill be weaponised to trick military.”[Online] Retrieved from \\nhttps://www.express.co.uk/news/science/1109783/deep -\\nfakes -ai- artificial -intelligencephotos -video -weaponised -\\nchina.  [Accessed:7 - march -2020]  \\n17. Marr, B., 2019. The Best (And Sc ariest) Examples Of AI -\\nEnabled Deepfakes. [online] Forbes. Available at: \\n<https://www.forbes.com /sites/bernardmarr/2019/07/22/the -\\nbest-and-scariest -examples -of-ai-enabled -\\ndeepfakes/#1697860d2eaf> [Accessed 5 March 2020].  \\n18. De keersmaecker,  J., Roets,  A. 2017. „Fake  news‟:  Incorrect,  \\nbut hard  to correct.  The role of cognitive  ability  on the impact  \\nof false information on social impressions. Intelligence, 65: \\n107–110. https://doi.org/10.1016/j.intell.2017.10.005  \\n19. Anderson, K. E. 2018. Getting acquainted with so cial \\nnetworks and apps: combating fake news on social media. \\nLibrary HiTech News, 35(3): 1 –6 \\n20. Brandon, John  , ”Terrifying  high-tech porn:  Creepy  \\n‟deepfake‟  videos are on the rise”. 20 February \\n2018.[Online]. Avaialble \\n:https://www.foxnews.com/tech/terrifyin g-high-tech-porn-\\ncreepy - deepfake -videos -are-on-the-rise.[Accessed:10 -\\nmarch -2020]  \\n21. ”Prepare, Don‟t Panic: Synthetic Media and Deepfakes”. \\nJune 2019.  [On- line]. Available : \\nhttps://lab.witness.org/projects/synthetic -media -and- deep -\\nfakes/.[Accessed:10 -march -2020]  \\n22. Schwartz, Oscar,”  You thought fake news was bad? Deep \\nfakes  are where truth goes to die”. The  Guardian. 14 \\nNovember 2018. [Online].Avaiable: \\nhttps://www.theguardian.com/technology/2018/nov/12/deep -\\nfakes - fake-news -.[Acc essed:15 -march -2020]  \\n23. PhD,  Sven  Charleer  . ”Family  fun with deepfakes.  Or how I \\ngot my wife onto the Tonight Show”. 17 May \\n2019.[Online].Avaiable: \\nhttps://towardsdatascience.com/family -fun-with-deepfakes -\\nor-how-i-got-my-wife-onto-the-tonight -show - \\na4454775c011.[Accessed:17 -march -2020]  \\n24. Clarke, Yvette D, ”H.R.3230 - 116th Congress (2019 -2020):  \\nDe- fending Each  and Every  Person  from  False  Appearances  \\nby Keeping Exploitation Subject to Accountability Act of \\n2019”. 28 June \\n2019.[Online].Avaiable:https:// www.congress.gov/bill/116th - \\ncongress/house -bill/3230.[Accessed:17 -march -2020]  \", metadata={'source': 'data/Deep_Insights_of_Deepfake_Technology_A_Review.pdf', 'page': 7}),\n"," Document(page_content='Deep Insights of Deepfake Technology : A Review  21 \\n25. ”What  Are Deepfakes and Why the Future  of Porn is \\nTerrifying”. Highsnobiety.  20 February 2018.[Online]. \\nAvaiable:https://www.highsnobiety.com/p/what -are-\\ndeepfakes -ai- porn/.[Acc essed:17 -march -2020]  \\n26. Roose, K., 2018. Here Come The Fake Videos, Too. [online] \\nNytimes.com.Available:<https://www.nytimes.com/2018/03/\\n04/technology/fake -videos -deepfakes.html> [Accessed 24 \\nMarch 2020].  \\n27. Ghoshal,  Abhimanyu  ,”Twitter,  Pornhub  and other  platfo rms \\nban AI -generated celebrity porn”. The Next Web. 7 February \\n2018.[Online].Avaiable:https://thenextweb.com/insider/2018/\\n02/07/twitter - pornhub -and-other -platforms -ban-ai-generated -\\ncelebrity - porn/.[Accessed:25 -march -2020]  \\n28. Khalid, A., 2019. Deepfake Video s Are A Far, Far Bigger \\nProblem For Women. [online] Quartz. Available at: \\n<https://qz.com/1723476/ deepfake -videos -feature -mostly -\\nporn-according -to-new-study -from -deeptrace -labs/> \\n[Accessed 25 March 2020].  \\n29. Dickson,  E. J. Dickson,  ”Deepfake  Porn  Is Still a Threat, \\nParticularly for K -Pop Stars”. 7 October 2019.[Online]. \\nAvaiable:https://www.rollingstone.com/culture/culture -\\nnews/deepfakes - nonconsensual -porn-study -kpop -\\n895605/.[Accessed:25 -march -2020]  \\n30. James Vincent,”New AI deepfake app creates nude images of \\nwomen in seconds”.June 27,2019.[Online].Avaiable: \\nhttps://www.theverge.com/2019/6/27/18760896/deepfake - \\nnude -ai-app-women -deepnude -non-consensual - \\npornography.[Accessed:25 -march -2020]  \\n31. Joseph Foley,”10 deepfake example that terrified and amused \\nthe internet” . 23March,  2020.[Online].  Available:  \\nhttps://www.creativebloq.com/features/deepfake - videos \\nexamples.[Accessed:30 -march -2020]  \\n32. “3 THINGS YOU NEED TO KNOW ABOUT AI -\\nPOWERED “DEEP FAKES” IN ART CULTURE” .17 \\nDecember,  2019.  [Online].  Avaiable: \\nhttps://cuseum.co m/blog/2019/12/17/3 -things -you-need -to-\\nknow -about - ai-powered -deep -fakes -in-art-amp-\\nculture.[Accessed:30 -march -2020]  \\n33. “dal ´ı lives (via artificial intelligence)” .11 May,  \\n2019,[Online]. Avaiable: https://thedali.org/exhibit/dali -\\nlives/.[Accessed:30 -march -2020] [34] “New deepfake AI \\ntech creates videos using one image”. 31May 2019.[Online]. \\nAvaiable: https://blooloop.com/news/samsung -ai- deepfake -\\nvideo -museum -technology/.[Accessed:2 -April -2020]  \\n34. “Positive Applications for Deepfake Technology.12 Novem - \\nber,201 9.[Online] Avaiable:”https://hackernoon.com/the -\\nlight- side-of-deepfakes -how-the-technology -can-be-used-\\nfor-good - 4hr32pp.[Accessed:2 -april-2020]  \\n35. Jedidiah  Francis,  Don‟t believe your eyes: Exploring the \\npositives andnegatives of deepfakes. 5 August 2019.[O nline]. \\nAvaiable:https://artificialintelligence - \\nnews.com/2019/08/05/dont -believe -your-eyes-exploring -the-\\npositives - and-negatives -of-deepfakes/.[Accessed:2 -April -\\n2020]  \\n36. Simon Chandler, Why Deepfakes Are A Net Positive For \\nHumanity.  \\n37. 9 March 2020.[Online]. \\nAvailable:https://www.forbes.com/sites/simonchandler/2020/03/09/ why -deepfakes -are-a-net-positive -for- humanity/334b. \\n[Accessed:2 -April -2020]  \\n38. Geraint  Rees, “Here‟s  how deepfake  technology  can actually  \\nbe a good thing”.25 November 2019.[Online]. Avaiable: \\nhttps://www.weforum.org/agenda/2019/11/advantages -of-\\nartificial - intelligence/.[Accessed:5 -April -2020]  \\n39. Patrick L. Plaisance Ph.D., “Ethics and “Synthetic Media” ”. \\n17 Septem - ber 2019. Avaiable: \\nhttps://www.psychologytoday.com/sg/blog/virtue - in-the-\\nmedia -world/201909/ethics -and-synthetic -\\nmedia.[Accessed:5 - April -2020]  \\n40. Cheng, Z., Sun, H., Takeuchi, M., and Katto, J. (2019). \\nEnergy compaction -based image compression using \\nconvolutional autoencoder. IEEE Transactions on \\nMultimedia. DOI: 10.1109/TMM.2019.2938345 . \\n41. Chorowski, J., Weiss, R. J., Bengio, S., and Oord, A. V. D. \\n(2019). Un - supervised speech representation learning using \\nwavenet autoencoders. IEEE/ACM Transactions on Audio, \\nSpeech, and Language Processing. 27(12), pp. 2041 -2053  \\n42. “FakeApp 2.2.0.”.[Online] Avaiable: \\nhttps://www.malavida.com/en/soft/fakeapp/ [Accessed:5 -\\nApril -2020]  \\n43. Alan Zucconi, “A Practical Tutorial for FakeApp”.18. \\nMarch,2018 [Online]. \\nAvaiable:https://www.alanzucconi.com/2018/03/14/a -\\npractical -tutorial - for-fakeapp/.[Accessed:7 -April -2020]  \\n44. Garrido,  P., Valgaerts,  L., Wu, C., Theobalt,  C. 2013. \\nReconstructing  Detailed Dynamic Face Geometry  from \\nMonocular Video.  ACM  Trans.  Graph.  32, 6, Article  158 \\n(November 2013), 10 pages. DOI = \\n10.1145/2508363.2508380 \\nhttp://doi.acm.org/10.1145/2508363.2508380.  \\n45. VALGAERTS, L., WU, C., BRUHN, A., SEIDEL, H. -P., \\nAND THEOBALT, C. 2012. Lightweight binocular facial \\nperformance capture under uncontrolled lighting. ACM TOG \\n(Proc. SIGGRAPH Asia) 31, 6, 187:1 –187:11.  \\n46. Cao, C., Hou,  Q., Zhou,  K. 2014.  Displaced  Dynamic \\nExpression Regression for Real -time Facial Tracking and  \\nAnimation.  ACM  Trans. Graph. 33, 4, Article  43 (July 2014),  \\n10 pages.  DOI = 10.1145/2601097.2601204 \\nhttp://doi.acm.org/10.1145/2601097.2601204.  \\n47. Cao, C., Bradl ey, D., Zhou, K., Beeler, T. 2015. Real -Time \\nHigh -Fidelity Facial Performance  Capture.  ACM  Trans.  \\nGraph.  34, 4, Article 46 (August 2015), 9 pages. DOI = \\n10.1145/2766943 http://doi.acm.org/10.1145/2766943.  \\n48. BEELER, T., HAHN, F., BRADLEY,  D., BICKEL, B., \\nBEAR DSLEY,  P., GOTSMAN, C., SUMNER, R. W., AND \\nGROSS, M. 2011. High -quality passive facial performance \\ncapture using anchor frames. ACM Trans. Graphics (Proc. \\nSIGGRAPH) 30, 75:1 –75:10.  \\n49. Thies,  J., Zollh  o¨fer,  M., Nießner,  M., Valgaerts,  L., \\nStamminger, M.,  Theobalt,  C. 2015.  Real-time Expression  \\nTransfer  for Facial Reenactment. ACM Trans. Graph. 34,  6, \\nArticle  183 (November 2015), 14 pages. DOI = \\n10.1145/2816795.2818056  \\nhttp://doi.acm.org/10.1145/2816795.2818056.  \\n50. WEISE, T., BOUAZIZ, S., LI, H., AND PAULY, M. 2011. \\nRealtime  performance -based facial animation. 77.  ', metadata={'source': 'data/Deep_Insights_of_Deepfake_Technology_A_Review.pdf', 'page': 8}),\n"," Document(page_content='22  Bahar Uddin Mahmud and Afsana Sharmin  \\n51. Thies,  J., Zollh  o¨fer,  M., Stamminger,  M., Theobalt,  C. \\nNießner,  M. (2019). Face2Face: Real -time face capture and \\nreenactment of RGB Commun. ACM, 62(1), 96 –104. \\ndoi:10.1145/3292039.  \\n52. F. Shi, H. -T. Wu, X. Tong, and J. Chai. Automatic \\nacquisition of high -fidelity facial performances using \\nmonocular videos. ACM TOG, 33(6):222, 2014.  \\n53. [53]P. Garrido, L. Valgaerts, H. Sarmadi, I. Steiner, K. \\nVaranasi, P. Perez,  and C. Theobalt. Vdub: Modifying face \\nvideo of a ctors for plausible visual alignment to a dubbed \\naudio track. In Computer Graphics Forum. Wiley -Blackwell, \\n2015.  \\n54. Supasorn Suwajanakorn, Steven M. Seitz, and Ira \\nKemelmacher - Shlizerman. 2017. Synthesizing Obama: \\nLearning Lip Sync from Audio. ACM Trans. Gra ph. 36, 4, \\nArticle 95 (July 2017), 13 pages. DOI: \\nhttp://dx.doi.org/10.1145/3072959.3073640  \\n55. Justus Thies, Michael Zollh  o¨fer, Christian Theobalt, Marc \\nStamminger, and Matthias Nießner. 2018. HeadOn: Real -\\ntime Reenactment of Human Portrait Videos. ACM Tran s. \\nGraph. 37, 4, Article 164 (August 2018),  13 pages. \\nhttps://doi.org/10.1145/3197517.3201350  \\n56. deepfakes/faceswap.[Online]. \\nAvaiable:https://github.com/deepfakes/faceswap.  \\n[Accessed:9 -April -2020]  \\n57. FaceSwap -GANS .[Online].Avaiable: \\nhttps://github.com/shaoanl u/faceswap - GAN.[Accessed:9 -\\nApril -2020]  \\n58. Keras -VGGFace: VGGFace implementation with Keras \\nframework.[Online]  Avaiable:  \\nhttps://github.com/rcmalli/keras - vggface.[Accessed:9 -April -\\n2020]  \\n59. ipazc/mtcnn.https://github.com/ipazc/mtcnn.[Online]. \\nAvaiable: https://github.com/ipazc/mtcnn.[Accessed:15 -\\nApril -2020]  \\n60. An Introduction to the Kalman Filter.[Online] Available : \\nhttp://www.cs.unc.edu/ welch/kalman/kalmanIntro. html  \\n61. jinfagang/faceswap -pytorch.[Online]. Avaiable  \\n:https://github.com/jinfagang/faceswap -pytorch. \\n[Accessed:15 -April - 2020]  \\n62. iperov/DeepFaceLab .[Online]. \\nAvaiable:https://github.com/iperov/DeepFaceLab. \\n[Accessed:17 -April -2020]  \\n63. DSSIM.[Online].Avaiable:https://github.com/keras -\\nteam/keras - \\ncontrib/blob/master/kerascontrib/losses/dssim.py.[Accessed:2\\n0-April - 2020]  \\n64. GitHub. 2019. Shaoanlu/Faceswap -GAN. [online] Available \\nat: <https://github.com/shaoanlu/faceswap -GAN> [Accessed \\n25 March 2020].  \\n65. P. Korshunov and S. Marcel, \"Vulnerability assessment and \\ndetection of Deepfake videos,\" 2019 International \\nConf erence on Biometrics (ICB), Crete, Greece, 2019, pp. 1 -\\n6, doi: 10.1109/ICB45273.2019.8987375.  \\n66. Wen, H. Han, and A. K. Jain. Face spoof detection with \\nimage distortion analysis. IEEE Transactions on Information \\nForensics and Security, 10(4):746 –761, April 2 015 67. \"VidTIMIT database\".[Online]. Avaiable: \\nhttp://conradsanderson.id.au/vidtimit/.[Accessed:22 -April -\\n2020]  \\n68. Deep face Recognition. In Xianghua Xie, Mark W. Jones, \\nand Gary K. L. Tam, editors, Proceedings of the British \\nMachine Vision Conference (BMVC), pag es 41.1 -41.12. \\nBMVA Press, September 2015.  \\n69. F. Schroff, D. Kalenichenko and J. Philbin, \"FaceNet: A \\nunified embedding for face recognition and clustering,\" 2015 \\nIEEE Conference on Computer Vision and Pattern \\nRecognition (CVPR), Boston, MA, 2015, pp. 815 -823, doi: \\n10.1109/CVPR.2015.7298682.  \\n70. Nguyen, T.T., Nguyen, C.M., Nguyen, D.T., Nguyen, D.T., \\n& Nahavandi, S. (2019). Deep Learning for Deepfakes \\nCreation and Detection. ArXiv, abs/1909.11573.  \\n71. Nguyen, H.H., Fang, F., Yamagishi, J., & Echizen, I. (2019). \\nMulti -task Learning For Detecting and Segmenting \\nManipulated Facial Images and Videos. ArXiv, \\nabs/1906.06876.  \\n72. Sabir, J. Cheng, A. Jaiswal, W. AbdAlmageed, I. Masi, and \\nP. Natarajan, “Recurrent Convolutional Strategies for Face \\nManipulation Detection in Videos.” 2019.  \\n73. Yuezun Li and Siwei Lyu. Exposing deepfake videos by \\ndetecting face warping artifacts. arXiv preprint \\narXiv:1811.00656, 2018  \\n74. Xin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakes \\nusing inconsistent head poses. In ICASSP, 2019. 2, 4, 5  \\n75. Li, M. Chang  and S. Lyu, \"In Ictu Oculi: Exposing AI \\nCreated Fake Videos by Detecting Eye Blinking,\" 2018 IEEE \\nInternational Workshop on Information Forensics and \\nSecurity (WIFS), Hong Kong, Hong Kong, 2018, pp. 1 -7, \\ndoi: 10.1109/WIFS.2018.8630787.  \\n76. Agarwal et al., 201 9] Shruti Agarwal, Hany Farid, Yuming \\nGu, Ming - ming He, Koki Nagano, and Hao Li. Protecting \\nworld leaders against deep fakes. In Proceedings of the IEEE \\nConference on Computer Vision and Pattern Recognition \\nWorkshops, pages 38 –45, 2019  \\n77. Huy H Nguyen, Junic hi Yamagishi, and Isao Echizen. \\nCapsule -forensics: Using capsule networks to detect forged \\nimages and videos. arXiv preprint arXiv:1810.11215, 2018.  \\n78. Andreas  R o¨ssler,  Davide  Cozzolino,  Luisa  Verdoliva,  \\nChristian  Riess, Justus Thies, and Matthias Nießner. 2019. \\nFaceForensics++: Learning  to Detect Manipulated Facial \\nImages. arXiv (2019)  \\n79. Güera and E. J. Delp, \"Deepfake Video Detection Using \\nRecurrent Neural Networks,\" 2018 15th IEEE International \\nConference on Advanced Video and Signal Based \\nSurveillance (AVS S), Auckland, New Zealand, 2018, pp. 1 -6, \\ndoi: 10.1109/AVSS.2018.8639163.  \\n80. Du, M., Pentyala, S.K., Li, Y., & Hu, X. (2019). Towards \\nGeneralizable Forgery Detection with Locality -aware \\nAutoEncoder. ArXiv, abs/1909.05999.  \\n81. Davide Cozzolino, , Justus Thies, And reas Rössler, Christian \\nRiess, Matthias Nie \\\\ssner, and Luisa Verdoliva. \\n\"ForensicTransfer: Weakly -supervised Domain Adaptation \\nfor Forgery Detection\".arXiv (2018).   ', metadata={'source': 'data/Deep_Insights_of_Deepfake_Technology_A_Review.pdf', 'page': 9}),\n"," Document(page_content='Deep Insights of Deepfake Technology : A Review  23 \\n82. Huang, Z. Liu, L. Van Der Maaten and K. Q. Weinberger, \\n\"Densely Connected Convolutional Networks,\" 2017 IEEE \\nConference on Computer Vision and Pattern Recognition \\n(CVPR), Honolulu, HI, 2017, pp. 2261 -2269, doi: \\n10.1109/CVPR.2017.243.  \\n83. Cho, Kyunghyun, Bart van Merrienboer, Çaglar Gülçehre, \\nDzmitry Bahdanau, Fethi Bougares, Holger Schwenk and \\nYoshua Bengio. “Learning Phrase Representations using \\nRNN Encoder -Decoder for Statistical Machine Translation.” \\nEMNLP (2014)  \\n84. Simonyan,  K., and Zisserman,  A. (2014).  Very  deep  convolu - \\ntional networks for large -scale image recognition. arXiv \\npreprint arXiv:1409.1556.  \\n85. He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual \\nlearning for image recognition. In Proceedings of the IEEE \\nConference on Computer Vision and Pattern Recognition (pp. \\n770-778).  \\n86. Sabour, S., Frosst, N., and Hinton, G. E. (2017).  Dynamic \\nrouting be tween capsules. In Advances in Neural \\nInformation Processing Systems (pp. 3856 -3866).  \\n87. Chih -Chung Hsu, Yi -Xiu Zhuang, & Chia -Yen Lee (2020). \\nDeep Fake Image Detection based on Pairwise \\nLearningApplied Sciences, 10, 370.  \\n88. S. Chopra, R. Ha dsell and Y. LeCun, \"Learning a similarity \\nmetric discriminatively, with application to face verification,\" \\n2005 IEEE Computer Society Conference on Computer \\nVision and Pattern Recognition (CVPR\\'05), San Diego, CA, \\nUSA, 2005, pp. 539 -546 vol. 1, doi: \\n10.11 09/CVPR.2005.202.  \\n89. Afchar, V. Nozick, J. Yamagishi and I. Echizen, \"MesoNet: a \\nCompact Facial Video Forgery Detection Network,\" 2018 \\nIEEE International Workshop on Information Forensics and \\nSecurity (WIFS), Hong Kong, Hong Kong, 2018, pp. 1 -7, \\ndoi: 10.1109/ WIFS.2018.8630761.  \\n90. Xuan X., Peng B., Wang W., Dong J. (2019) On the \\nGeneralization of GAN Image Forensics. In: Sun Z., He R., Feng J., Shan S., Guo Z. (eds) Biometric Recognition. CCBR \\n2019. Lecture Notes in Computer Science, vol 11818. \\nSpringer, Cham. htt ps://doi.org/10.1007/978 -3-030-31456 -\\n9_15  \\n91. Y. Zhang, L. Zheng and V. L. L. Thing, \"Automated face \\nswapping and its detection,\" 2017 IEEE 2nd International \\nConference on Signal and Image Processing (ICSIP), \\nSingapore, 2017, pp. 15 -19, doi: \\n10.1109/SIPROCESS. 2017.8124497.  \\n92. Matern, C. Riess and M. Stamminger, \"Exploiting Visual \\nArtifacts to Expose Deepfakes and Face Manipulations,\" \\n2019 IEEE Winter Applications of Computer Vision \\nWorkshops (WACVW), Waikoloa Village, HI, USA, 2019, \\npp. 83 -92, doi: 10.1109/WACVW.2 019.00020.  \\n93. arissa  Koopman,  Andrea  Macarulla  Rodriguez,  and Zeno  \\nGeradts.Detection of deepfake video manipulation. In \\nConference: IMVIP, 2018 .  \\n94. Ruben Tolosana, DeepFakes and Beyond A Survey of Face \\nManipulation and Fake Detection, pp. 1 -15, 2020  \\n95. Brian Dolha nsky, The Deepfake Detection Challenge \\n(DFDC) Preview Dataset, Deepfake Detection Challenge, pp. \\n1-4, 2019.  \\n96. Chesney, R. and K. Citron, D., 2018. Disinformation On \\nSteroids: The Threat Of Deep Fakes. [online] Council on \\nForeign Relations. Available at: \\n<https://www.cfr.org/report/deep -fake-disinformation -\\nsteroids> [Accessed 2 May 2020].  \\n97. Figueira, A., Oliveira, L. 2017. The current state of fake \\nnews:  challenges and opportunities. Procedia Computer \\nScience, 121: 817 –825. \\nhttps://doi.org/10.1016/j.procs.2017.1 1.106  \\n98. Zannettou, S., Sirivianos, M., Blackburn, J., Kourtellis, N. \\n2019. The  Web of False Information: Rumors, Fake News, \\nHoaxes, Clickbait, and Various Other Shenanigans. Journal \\nof Data and Information Quality, 1(3): Article No. 10. \\nhttps://doi.org/10.11 45/3309699  \\n \\n  ', metadata={'source': 'data/Deep_Insights_of_Deepfake_Technology_A_Review.pdf', 'page': 10}),\n"," Document(page_content='24  Bahar Uddin Mahmud and Afsana Sharmin  \\n ', metadata={'source': 'data/Deep_Insights_of_Deepfake_Technology_A_Review.pdf', 'page': 11})]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["docs"]},{"cell_type":"markdown","metadata":{"id":"HT6lpgvA8347"},"source":["embeddings"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5866,"status":"ok","timestamp":1707572559290,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"klR4YV7cATpo","outputId":"234a3a35-6235-43e4-b3c3-5f6a1b5c0a7a"},"outputs":[{"data":{"text/plain":["['/bin/bash: line 1: fg: no job control']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["!!%pip install InstructorEmbedding"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12542,"status":"ok","timestamp":1707572571828,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"JU34I0P7Ab9h","outputId":"00a2dbc7-7bac-4c26-f647-21a3f6af715a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting sentence_transformers\n","  Downloading sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\n","Collecting transformers<5.0.0,>=4.32.0 (from sentence_transformers)\n","  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from sentence_transformers) (4.66.1)\n","Collecting torch>=1.11.0 (from sentence_transformers)\n","  Using cached torch-2.2.0-cp39-cp39-manylinux1_x86_64.whl.metadata (25 kB)\n","Requirement already satisfied: numpy in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from sentence_transformers) (1.26.4)\n","Collecting scikit-learn (from sentence_transformers)\n","  Downloading scikit_learn-1.4.0-1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Collecting scipy (from sentence_transformers)\n","  Downloading scipy-1.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nltk in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from sentence_transformers) (3.8.1)\n","Collecting sentencepiece (from sentence_transformers)\n","  Downloading sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n","\u001b[?25hCollecting huggingface-hub>=0.15.1 (from sentence_transformers)\n","  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n","Collecting Pillow (from sentence_transformers)\n","  Downloading pillow-10.2.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n","Collecting filelock (from huggingface-hub>=0.15.1->sentence_transformers)\n","  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: fsspec>=2023.5.0 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.2.0)\n","Requirement already satisfied: requests in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.9.0)\n","Requirement already satisfied: packaging>=20.9 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n","Collecting sympy (from torch>=1.11.0->sentence_transformers)\n","  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n","Requirement already satisfied: networkx in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n","Collecting jinja2 (from torch>=1.11.0->sentence_transformers)\n","  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence_transformers)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting triton==2.2.0 (from torch>=1.11.0->sentence_transformers)\n","  Downloading triton-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n","  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: regex!=2019.12.17 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n","Collecting tokenizers<0.19,>=0.14 (from transformers<5.0.0,>=4.32.0->sentence_transformers)\n","  Downloading tokenizers-0.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.32.0->sentence_transformers)\n","  Downloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: click in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from nltk->sentence_transformers) (8.1.7)\n","Requirement already satisfied: joblib in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from nltk->sentence_transformers) (1.3.2)\n","Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence_transformers)\n","  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n","Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence_transformers)\n","  Downloading MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.2.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n","Collecting mpmath>=0.19 (from sympy->torch>=1.11.0->sentence_transformers)\n","  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.2.0-cp39-cp39-manylinux1_x86_64.whl (755.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:06\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n","\u001b[?25hDownloading triton-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n","\u001b[?25hDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading pillow-10.2.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading scikit_learn-1.4.0-1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading scipy-1.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.5/38.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hUsing cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n","Downloading tokenizers-0.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n","Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece, mpmath, threadpoolctl, sympy, scipy, safetensors, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, MarkupSafe, filelock, triton, scikit-learn, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, sentence_transformers\n","Successfully installed MarkupSafe-2.1.5 Pillow-10.2.0 filelock-3.13.1 huggingface-hub-0.20.3 jinja2-3.1.3 mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 safetensors-0.4.2 scikit-learn-1.4.0 scipy-1.12.0 sentence_transformers-2.3.1 sentencepiece-0.1.99 sympy-1.12 threadpoolctl-3.2.0 tokenizers-0.15.1 torch-2.2.0 transformers-4.37.2 triton-2.2.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install sentence_transformers"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8844,"status":"ok","timestamp":1707572580665,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"9r1dka11Bnyb","outputId":"61406a68-5cd4-4c65-a5d6-ccfc871e60fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: faiss-cpu in /home/pallavi/Downloads/ENTER/envs/llm/lib/python3.9/site-packages (1.7.4)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install faiss-cpu"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1707572580666,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"sishfzKyCCkz"},"outputs":[],"source":["from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":3763,"status":"ok","timestamp":1707572584423,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"5c5i1YyfCEsj"},"outputs":[{"name":"stderr","output_type":"stream","text":["modules.json: 100%|██████████| 349/349 [00:00<00:00, 32.5kB/s]\n","config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 12.5kB/s]\n","README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 5.53MB/s]\n","sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 37.2kB/s]\n","config.json: 100%|██████████| 571/571 [00:00<00:00, 379kB/s]\n","pytorch_model.bin: 100%|██████████| 438M/438M [02:46<00:00, 2.63MB/s] \n","tokenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 40.9kB/s]\n","vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 492kB/s]\n","tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 637kB/s]\n","special_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 175kB/s]\n","1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 21.8kB/s]\n"]}],"source":["\n","embeddings =  HuggingFaceEmbeddings()"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":55103,"status":"ok","timestamp":1707572639523,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"pHNiqOZM8347"},"outputs":[],"source":["vector_db =FAISS.from_documents(documents=docs, embedding=embeddings)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":448,"status":"ok","timestamp":1707573402739,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"wPJSDVMf8347"},"outputs":[],"source":["retriever  = vector_db.as_retriever()\n","rdocs=retriever.get_relevant_documents(\"what do you understand by word 'GAN'?\")"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"LgI-3OTwTlf3"},"outputs":[{"data":{"text/plain":["[Document(page_content='Fig. 4. The GAN architecture consisting of a generator and a discrim-\\ninator, and each can be implemented by a neural network. The entire\\nsystem can be trained with backpropagation that allows both networks\\nto improve their capabilities.\\nbyGand real images x. The discriminator Dis trained\\nto improve its classiﬁcation capability, i.e., to maximize\\nD(x), which represents the probability that xis a real\\nimage rather than a fake image generated by G. On the\\nother hand, Gis trained to minimize the probability that\\nits outputs are classiﬁed by Das synthetic images, i.e.,\\nto minimize 1−D(G(z)). This is a minimax game be-\\ntween two players DandGthat can be described by the\\nfollowing value function [4]:\\nmin\\nGmax\\nDV(D,G)=Ex∼pdata(x)[logD(x)]\\n+Ez∼pz(z)[log(1−D(G(z)))] (1)\\nAfter su ﬃcient training, both networks improve their\\ncapabilities, i.e., the generator Gis able to produce im-\\nages that are really similar to real images while the dis-\\ncriminator Dis highly capable of distinguishing fake\\nimages from real ones.\\nTable 1 presents a summary of popular deepfake tools\\nand their typical features. Among them, a prominent\\nmethod for face synthesis based on a GAN model,\\nnamely StyleGAN, was introduced in [51]. StyleGAN\\nis motivated by style transfer [61] with a special gen-\\nerator network architecture that is able to create realis-\\ntic face images. In a traditional GAN model, e.g., the\\nprogressive growing of GAN (PGGAN) [62], the signal\\nnoise (latent code) is fed to the input layer of a feed-\\nforward network that represents the generator. In Style-\\nGAN, there are two networks constructed and linked to-\\ngether, a mapping network fand a synthesis network g.\\nThe latent code z∈Zis ﬁrst converted to w∈W(where\\nWis an intermediate latent space) through a non-linear\\nfunction f:Z→W, which is characterized by a neural\\nnetwork (i.e., the mapping network) consisting of sev-eral fully connected layers. Using an a ﬃne tranforma-\\ntion, the intermediate representation wis specialized to\\nstyles y=(ys,yb) that will be fed to the adaptive in-\\nstance normalization (AdaIN) operations, speciﬁed as:\\nAdaIN( xi,y)=ys,ixi−µ(xi)\\nσ(xi)+yb,i (2)\\nwhere each feature map xiis normalized separately. The\\nStyleGAN generator architecture allows controlling the\\nimage synthesis by modifying the styles via di ﬀerent\\nscales. In addition, instead of using one random latent\\ncode during training, this method uses two latent codes\\nto generate a given proportion of images. More speciﬁ-\\ncally, two latent codes z1andz2are fed to the mapping\\nnetwork to create respectively w1andw2that control the\\nstyles by applying w1before and w2after the crossover\\npoint. Fig. 5 demonstrates examples of images cre-\\nated by mixing two latent codes at three di ﬀerent scales\\nwhere each subset of styles controls separate meaning-\\nful high-level attributes of the image. In other words,\\nthe generator architecture of StyleGAN is able to learn\\nseparation of high-level attributes (e.g., pose and iden-\\ntity when trained on human faces) and enables intuitive,\\nscale-speciﬁc control of the face synthesis.\\nFig. 5. Examples of mixing styles using StyleGAN: the output im-\\nages are generated by copying a speciﬁed subset of styles from source\\nB and taking the rest from source A. a) Copying coarse styles from\\nsource B will generate images that have high-level aspects from\\nsource B and all colors and ﬁner facial features from source A; b)\\nif copying the styles of middle resolutions from B, the output images\\nwill have smaller scale facial features from B and preserve the pose,\\ngeneral face shape, and eyeglasses from A; c) if copying the ﬁne styles\\nfrom source B, the generated images will have the color scheme and\\nmicrostructure of source B [51].\\n5', metadata={'source': 'data/1909.11573.pdf', 'page': 4}),\n"," Document(page_content='Methods Classiﬁers /\\nTechniquesKey Features Dealing\\nwithDatasets Used\\nPreprocessing\\ncombined with\\ndeep network\\n[74]DCGAN,\\nWGAN-GP and\\nPGGAN.- Enhance generalization ability of deep learning mod-\\nels to detect GAN generated images.\\n- Remove low level features of fake images.\\n- Force deep networks to focus more on pixel level sim-\\nilarity between fake and real images to improve gener-\\nalization ability.Images - Real dataset: CelebA-HQ [62], including high qual-\\nity face images of 1024x1024 resolution.\\n- Fake datasets: generated by DCGAN [88], WGAN-\\nGP [90] and PGGAN [62].\\nAnalyzing con-\\nvolutional traces\\n[151]KNN, SVM, and\\nlinear discrim-\\ninant analysis\\n(LDA)Using expectation-maximization algorithm to extract\\nlocal features pertaining to convolutional generative\\nprocess of GAN-based image deepfake generators.Images Authentic images from CelebA and correspond-\\ning deepfakes are created by ﬁve di ﬀerent GANs\\n(group-wise deep whitening-and-coloring transfor-\\nmation GDWCT [152], StarGAN [153], AttGAN\\n[154], StyleGAN [51], StyleGAN2 [155]).\\nBag of words\\nand shallow\\nclassiﬁers [78]SVM, RF, MLP Extract discriminant features using bag of words\\nmethod and feed these features into SVM, RF and MLP\\nfor binary classiﬁcation: innocent vs fabricated.Images The well-known LFW face database [156], containing\\n13,223 images with resolution of 250x250.\\nPairwise learn-\\ning [85]CNN con-\\ncatenated to\\nCFFNTwo-phase procedure: feature extraction using CFFN\\nbased on the Siamese network architecture [86] and\\nclassiﬁcation using CNN.Images - Face images: real ones from CelebA [87], and\\nfake ones generated by DCGAN [88], WGAN [89],\\nWGAN-GP [90], least squares GAN [91], and PG-\\nGAN [62].\\n- General images: real ones from ILSVRC12 [92],\\nand fake ones generated by BIGGAN [93], self-\\nattention GAN [94] and spectral normalization GAN\\n[95].\\nDefenses against\\nadversarial per-\\nturbations in\\ndeepfakes [157]VGG [65] and\\nResNet [118]- Introduce adversarial perturbations to enhance deep-\\nfakes and fool deepfake detectors.\\n- Improve accuracy of deepfake detectors using Lips-\\nchitz regularization and deep image prior techniques.Images 5,000 real images from CelebA [87] and 5,000 fake\\nimages created by the “Few-Shot Face Translation\\nGAN” method [158].\\nFace X-ray\\n[159]CNN - Try to locate the blending boundary between the target\\nand original faces instead of capturing the synthesized\\nartifacts of speciﬁc manipulations.\\n- Can be trained without fake images.Images FaceForensics ++[105], DeepfakeDetection (DFD)\\n[106], DFDC [109] and Celeb-DF [107].\\nUsing common\\nartifacts of\\nCNN-generated\\nimages [160]ResNet-50 [118]\\npre-trained with\\nImageNet [92]Train the classiﬁer using a large number of fake images\\ngenerated by a high-performing unconditional GAN\\nmodel, i.e., PGGAN [62] and evaluate how well the\\nclassiﬁer generalizes to other CNN-synthesized images.Images A new dataset of CNN-generated images, namely\\nForenSynths, consisting of synthesized images from\\n11 models such as StyleGAN [51], super-resolution\\nmethods [161] and FaceForensics ++[105].\\nUsing convolu-\\ntional traces on\\nGAN-based im-\\nages [162]KNN, SVM, and\\nLDATraining the expectation-maximization algorithm [163]\\nto detect and extract discriminative features via a ﬁn-\\ngerprint that represents the convolutional traces left by\\nGANs during image generation.Images A dataset of images generated by ten GAN models,\\nincluding CycleGAN [164], StarGAN [153], AttGAN\\n[154], GDWCT [152], StyleGAN [51], StyleGAN2\\n[155], PGGAN [62], FaceForensics ++[105], IMLE\\n[165], and SPADE [42].\\nUsing deep fea-\\ntures extracted\\nby CNN [100]A new CNN\\nmodel, namely\\nSCnetThe CNN-based SCnet is able to automatically learn\\nhigh-level forensics features of image data thanks to a\\nhierarchical feature extraction block, which is formed\\nby stacking four convolutional layers.Images A dataset of 321,378 face images, created by apply-\\ning the Glow model [101] to the CelebA face image\\ndataset [87].\\ncontents is also quicker thanks to the development of\\nsocial media platforms [166]. Sometimes deepfakes do\\nnot need to be spread to massive audience to cause detri-\\nmental e ﬀects. People who create deepfakes with ma-\\nlicious purpose only need to deliver them to target au-\\ndiences as part of their sabotage strategy without using\\nsocial media. For example, this approach can be utilized\\nby intelligence services trying to inﬂuence decisions\\nmade by important people such as politicians, leading to\\nnational and international security threats [167]. Catch-\\ning the deepfake alarming problem, research commu-\\nnity has focused on developing deepfake detection algo-\\nrithms and numerous results have been reported. This\\npaper has reviewed the state-of-the-art methods and a\\nsummary of typical approaches is provided in Table 2.\\nIt is noticeable that a battle between those who use ad-\\nvanced machine learning to create deepfakes with those\\nwho make e ﬀort to detect deepfakes is growing.Deepfakes’ quality has been increasing and the per-\\nformance of detection methods needs to be improved\\naccordingly. The inspiration is that what AI has broken\\ncan be ﬁxed by AI as well [168]. Detection methods are\\nstill in their early stage and various methods have been\\nproposed and evaluated but using fragmented datasets.\\nAn approach to improve performance of detection meth-\\nods is to create a growing updated benchmark dataset of\\ndeepfakes to validate the ongoing development of detec-\\ntion methods. This will facilitate the training process of\\ndetection models, especially those based on deep learn-\\ning, which requires a large training set [108].\\nImproving performance of deepfake detection meth-\\nods is important, especially in cross-forgery and cross-\\ndataset scenarios. Most detection models are designed\\nand evaluated in the same-forgery and in-dataset exper-\\niments, which do not ensure their generalization capa-\\nbility. Some previous studies have addressed this issue,\\n12', metadata={'source': 'data/1909.11573.pdf', 'page': 11}),\n"," Document(page_content='16  Bahar Uddin Mahmud and Afsana Sharmin  \\nmore combination result with original image. Most used \\ntool for making Deepfake videos is “DeepFaceLab” [61], \\nthat runs much faster than previous version. It has more \\ninteractive converter. “DFaker” [62] is another tool that \\nused DSSIM loss func tion [63] to reconstruct face.  Most of \\nthe face swapping tools use the concepts of Generative \\nadversarial networks(GANS). From [64, fig. 1 ], it is the \\nvisual diagram of encoder in GANS. It has the ability to \\nextract deep information from image, in encodin g stage it \\nextracts the facial expressions of two individual. Then it \\nanalyze the common features between two faces and \\nmemorizes it. After that, in decoding stage in [64,fig.2], it \\ndecodes the information of two individuals. And in [64,fig. \\n3] shows the w orkings of discriminator that used to make \\ndecision of authenticity of features of every given data. \\nCopy the source features to target features is most tedious \\ntask in face swapping and it is done by autoencoder after \\nproper training. A implicit layer ins ide the encoder trained to generate the code to represent input. And auto encoder \\ncomprise of 2 slice encoder that represent input and decoder \\nthat generates the reconstruction. As represent in [64,fig. 4] \\nand face swapping tools described here mostly used  \\nautoencoder technique. The main objectives of autoencoder \\nis to handle MAE loss(reconstruction loss), adversarial loss( \\nperforms by Discriminator) and perceptual loss  (which \\noptimize similarity between source image and target \\nimage).  \\n3. Deepfake Detection  \\nIt is often difficult  and sometimes impossible to detect \\nDeepfake contents by a human being with untrained eye s. A \\ngood level of expertise is needed to detect irregularities in \\nDeep fake videos. Till now several approaches have been \\nproposed including machin e detection, forensics, \\nauthentication as well as regulation to combat Deepfake.  \\n \\nTable 1: Summary of remarkable deepfake related articles  \\n \\n', metadata={'source': 'data/Deep_Insights_of_Deepfake_Technology_A_Review.pdf', 'page': 3}),\n"," Document(page_content='Table 1: Summary of notable deepfake tools\\nTools Links Key Features\\nFaceswap https: //github.com /deepfakes /faceswap - Using two encoder-decoder pairs.\\n- Parameters of the encoder are shared.\\nFaceswap-GAN https: //github.com /shaoanlu /faceswap-GAN Adversarial loss and perceptual loss (VGGface) are added to an auto-encoder architecture.\\nFew-Shot Face\\nTranslationhttps: //github.com /shaoanlu /fewshot-face-\\ntranslation-GAN- Use a pre-trained face recognition model to extract latent embeddings for GAN process-\\ning.\\n- Incorporate semantic priors obtained by modules from FUNIT [41] and SPADE [42].\\nDeepFaceLab https: //github.com /iperov /DeepFaceLab - Expand from the Faceswap method with new models, e.g. H64, H128, LIAEF128, SAE\\n[43].\\n- Support multiple face extraction modes, e.g. S3FD, MTCNN, dlib, or manual [43].\\nDFaker https: //github.com /dfaker /df - DSSIM loss function [44] is used to reconstruct face.\\n- Implemented based on Keras library.\\nDeepFake tf https: //github.com /StromWine /DeepFake tf Similar to DFaker but implemented based on tensorﬂow.\\nAvatarMe https: //github.com /lattas /AvatarMe - Reconstruct 3D faces from arbitrary “in-the-wild” images.\\n- Can reconstruct authentic 4K by 6K-resolution 3D faces from a single low-resolution\\nimage [45].\\nMarioNETte https: //hyperconnect.github.io /MarioNETte - A few-shot face reenactment framework that preserves the target identity.\\n- No additional ﬁne-tuning phase is needed for identity adaptation [46].\\nDiscoFaceGAN https: //github.com /microsoft /DiscoFaceGAN - Generate face images of virtual people with independent latent variables of identity, ex-\\npression, pose, and illumination.\\n- Embed 3D priors into adversarial learning [47].\\nStyleRig https: //gvv.mpi-inf.mpg.de /projects /StyleRig - Create portrait images of faces with a rig-like control over a pretrained and ﬁxed Style-\\nGAN via 3D morphable face models.\\n- Self-supervised without manual annotations [48].\\nFaceShifter https: //lingzhili.com /FaceShifterPage - Face swapping in high-ﬁdelity by exploiting and integrating the target attributes.\\n- Can be applied to any new face pairs without requiring subject speciﬁc training [49].\\nFSGAN https: //github.com /YuvalNirkin /fsgan - A face swapping and reenactment model that can be applied to pairs of faces without\\nrequiring training on those faces.\\n- Adjust to both pose and expression variations [50].\\nStyleGAN https: //github.com /NVlabs /stylegan - A new generator architecture for GANs is proposed based on style transfer literature.\\n- The new architecture leads to automatic, unsupervised separation of high-level attributes\\nand enables intuitive, scale-speciﬁc control of the synthesis of images [51].\\nFace2Face https: //justusthies.github.io /posts/face2face / - Real-time facial reenactment of monocular target video sequence, e.g. Youtube video.\\n- Animate the facial expressions of the target video by a source actor and re-render the\\nmanipulated output video in a photo-realistic fashion [52].\\nNeural Textures https: //github.com /SSRSGJYD /NeuralTexture - Feature maps that are learned as part of the scene capture process and stored as maps on\\ntop of 3D mesh proxies.\\n- Can coherently re-render or manipulate existing video content in both static and dynamic\\nenvironments at real-time rates [53].\\nTransformable\\nBottleneck\\nNetworkshttps: //github.com /kyleolsz /TB-Networks - A method for ﬁne-grained 3D manipulation of image content.\\n- Apply spatial transformations in CNN models using a transformable bottleneck frame-\\nwork [54].\\n“Do as I Do”\\nMotion\\nTransfergithub.com /carolineec /EverybodyDanceNow - Automatically transfer the motion from a source to a target person by learning a video-\\nto-video translation.\\n- Can create a motion-synchronized dancing video with multiple subjects [55].\\nNeural V oice\\nPuppetryhttps: //justusthies.github.io /posts/neural-voice-\\npuppetry- A method for audio-driven facial video synthesis.\\n- Synthesize videos of a talking head from an audio sequence of another person using 3D\\nface representation. [56].\\n4', metadata={'source': 'data/1909.11573.pdf', 'page': 3})]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["rdocs"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":687,"status":"ok","timestamp":1707573643913,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"bnPgIZp2Tmgz"},"outputs":[],"source":["from langchain.chains import RetrievalQA"]},{"cell_type":"markdown","metadata":{"id":"lFtdHH91VFLw"},"source":["problems\n","\n","1. generate answer from its knowlegde rather than from given embeddings.\n","2. if the answer is not in documents , it will create on its own.\n","\n","\n","To solve this issue, use prompt template"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":405,"status":"ok","timestamp":1707574832599,"user":{"displayName":"Pallavi Devi","userId":"13979128758680411566"},"user_tz":-330},"id":"0GjdxcQYUzFi"},"outputs":[],"source":["from langchain.prompts import PromptTemplate\n","\n","prompt_template =\"\"\" Given the following context and question, generate an answer based on this context.In the answer try to provide as much as text\n","possible from \"response\" section in the source document. If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n","CONTEXT : {context}\n","\n","QUESTION : {question}\n","\n","\"\"\"\n","\n","PROMPT = PromptTemplate(\n","    template = prompt_template,\n","    input_variables = [\"context\",\"question\"]\n",")\n","\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"CugIediKZCmY"},"outputs":[],"source":["chain = RetrievalQA.from_chain_type(llm=llm,\n","            chain_type = \"stuff\",\n","            retriever = retriever,\n","            input_key = \"query\",\n","            return_source_documents = True,\n","                                    chain_type_kwargs = {'prompt':PROMPT}\n","\n","                                    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"dl","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
